---
date: 2025-08-11 02:00:00
layout: post
title: ğŸš€ Unslothë¡œ LLM ëª¨ë¸ í•™ìŠµí•˜ê¸° - 2ë°° ë¹ ë¥´ê³  80% ë©”ëª¨ë¦¬ ì ˆì•½í•˜ëŠ” í˜ì‹ ì  ë°©ë²•!
subtitle: ì†Œí˜• GPUë¡œë„ ëŒ€í˜• ì–¸ì–´ëª¨ë¸ íŒŒì¸íŠœë‹ì´ ê°€ëŠ¥í•œ ë§ˆë²• ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬
description: >-
  Unslothë¥¼ í™œìš©í•œ íš¨ìœ¨ì ì¸ LLM ëª¨ë¸ í•™ìŠµ ì™„ì „ ê°€ì´ë“œ! 2ë°° ë¹ ë¥¸ ì†ë„ì™€ 80% ë©”ëª¨ë¦¬ ì ˆì•½ìœ¼ë¡œ ëˆ„êµ¬ë‚˜ ì‰½ê²Œ AI ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆì–´ìš”. ì„¤ì¹˜ë¶€í„° ì‹¤ì „ í™œìš©ê¹Œì§€ ëª¨ë“  ê³¼ì •ì„ ì¹œì ˆí•˜ê²Œ ì„¤ëª…ë“œë¦½ë‹ˆë‹¤!
image: /assets/img/post/20250811/unsloth.avif
optimized_image: /assets/img/post/20250811/unsloth.avif
category: AI
tags:
  - unsloth
  - llm-training
  - machine-learning
  - fine-tuning
  - efficiency
  - gpu-optimization
author: welnai
mermaid: true
---

ì•ˆë…•í•˜ì„¸ìš”, ê¸°ìˆ  ë„íŒŒë¯¼ ì¤‘ë…ì **Welnai**ì˜ˆìš”! ğŸ¤–âœ¨

ì˜¤ëŠ˜ì€ ì •ë§ì •ë§ í¥ë¯¸ì§„ì§„í•œ ì†Œì‹ì„ ê°€ì ¸ì™”ì–´ìš”! ë°”ë¡œ **Unsloth**ë¼ëŠ” í˜ì‹ ì ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ë°ìš”, ì´ ì¹œêµ¬ë¥¼ ì‚¬ìš©í•˜ë©´ ëŒ€í˜• ì–¸ì–´ëª¨ë¸(LLM) í•™ìŠµì„ **2ë°° ë” ë¹ ë¥´ê²Œ**, ê·¸ë¦¬ê³  **80%ë‚˜ ì ì€ ë©”ëª¨ë¦¬**ë¡œ í•  ìˆ˜ ìˆë‹¤ë‹ˆ! ğŸ˜±ğŸ’«

ì†Œí˜• GPUë¥¼ ê°€ì§„ ê°œë°œìë¶„ë“¤ë„ ì´ì œ ê±±ì • ì—†ì–´ìš”! í•¨ê»˜ ì´ ë§ˆë²• ê°™ì€ ë„êµ¬ì˜ ì„¸ê³„ë¡œ ë– ë‚˜ë³¼ê¹Œìš”? ğŸ‰

## ğŸŒŸ Unslothê°€ ë­”ê°€ìš”? 

<div class="mermaid">
graph TB
    A[ê¸°ì¡´ LLM í•™ìŠµ] --> B[ğŸ˜° ëŠë¦° ì†ë„]
    A --> C[ğŸ’¸ ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©]
    A --> D[ğŸ”¥ GPU ê³¼ë¶€í•˜]
    
    E[Unsloth ì‚¬ìš©] --> F[âš¡ 2ë°° ë¹ ë¥¸ ì†ë„]
    E --> G[ğŸ’° 80% ë©”ëª¨ë¦¬ ì ˆì•½]
    E --> H[ğŸ¯ ì •í™•ë„ ìœ ì§€]
    
    style E fill:#e1f5fe
    style F fill:#c8e6c9
    style G fill:#c8e6c9
    style H fill:#c8e6c9
</div>

**Unsloth**ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ëŒ€í˜• ì–¸ì–´ëª¨ë¸ì˜ íŒŒì¸íŠœë‹ì„ í˜ì‹ ì ìœ¼ë¡œ ìµœì í™”í•´ì£¼ëŠ” ë„êµ¬ì˜ˆìš”! ğŸš€

### âœ¨ í•µì‹¬ íŠ¹ì§•ë“¤

- **ğŸƒâ€â™€ï¸ 2ë°° ë¹ ë¥¸ í•™ìŠµ ì†ë„**: ê¸°ì¡´ ëŒ€ë¹„ í›¨ì”¬ ë¹ ë¥¸ í•™ìŠµì´ ê°€ëŠ¥í•´ìš”!
- **ğŸ’¾ 70-80% ë©”ëª¨ë¦¬ ì ˆì•½**: ì‘ì€ GPUë¡œë„ í° ëª¨ë¸ í•™ìŠµ ê°€ëŠ¥!
- **ğŸ¯ 100% ì •í™•ë„ ìœ ì§€**: ê·¼ì‚¬ì¹˜ ë°©ë²• ì—†ì´ ì›ë³¸ ì„±ëŠ¥ ê·¸ëŒ€ë¡œ!
- **ğŸ”§ OpenAI Triton ê¸°ë°˜**: ìµœì í™”ëœ ì»¤ë„ë¡œ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±!
- **ğŸŒ ê´‘ë²”ìœ„í•œ í˜¸í™˜ì„±**: Llama, Gemma, Qwen3, Mistral ë“± ì§€ì›!

## ğŸ› ï¸ ì„¤ì¹˜í•˜ê¸°

ì •ë§ ê°„ë‹¨í•´ìš”! ë¦¬ëˆ…ìŠ¤ í™˜ê²½ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë§Œ ì‹¤í–‰í•˜ë©´ ë¼ìš”:

```bash
pip install unsloth
```

> **ğŸ“ ì°¸ê³ **: í˜„ì¬ Linuxì™€ Windowsë¥¼ ì§€ì›í•˜ë©°, 2018ë…„ ì´í›„ NVIDIA GPUê°€ í•„ìš”í•´ìš”!

## ğŸ¯ ê¸°ë³¸ ì‚¬ìš©ë²• - ì²« ë²ˆì§¸ ëª¨ë¸ í•™ìŠµí•´ë³´ê¸°!

ì´ì œ ì‹¤ì œë¡œ ëª¨ë¸ì„ í•™ìŠµí•´ë³¼ ì‹œê°„ì´ì—ìš”! ì„¤ë ˜ ê°€ë“í•œ ì²« ë²ˆì§¸ ë‹¨ê³„ë¶€í„° ì°¨ê·¼ì°¨ê·¼ ë”°ë¼í•´ë³´ì„¸ìš”! ğŸ’ª

<div class="mermaid">
flowchart TD
    A[ëª¨ë¸ ì„ íƒ] --> B[Unslothë¡œ ë¡œë“œ]
    B --> C[PEFT ì„¤ì •]
    C --> D[ë°ì´í„°ì…‹ ì¤€ë¹„]
    D --> E[íŠ¸ë ˆì´ë„ˆ ì„¤ì •]
    E --> F[í•™ìŠµ ì‹œì‘!]
    F --> G[ëª¨ë¸ ì €ì¥]
    
    style A fill:#fff3e0
    style B fill:#e8f5e8
    style C fill:#e3f2fd
    style D fill:#fce4ec
    style E fill:#f3e5f5
    style F fill:#e0f2f1
    style G fill:#fff8e1
</div>

### 1ï¸âƒ£ ëª¨ë¸ ë¡œë“œí•˜ê¸°

```python
from unsloth import FastLanguageModel, FastModel
from trl import SFTTrainer, SFTConfig
import torch

# ğŸ¯ ì‚¬ì „ ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/gemma-2-2b-it",  # ì˜ˆì‹œ: Gemma 2B ëª¨ë¸
    max_seq_length = 2048,                  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
    dtype = None,                           # ìë™ ê°ì§€
    load_in_4bit = True,                    # 4ë¹„íŠ¸ ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½!
)
```

### 2ï¸âƒ£ íŒŒì¸íŠœë‹ì„ ìœ„í•œ PEFT ì„¤ì •

```python
# ğŸ”§ PEFT (Parameter Efficient Fine-Tuning) ì„¤ì •
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,                    # LoRA attention dimension
    target_modules = [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ],
    lora_alpha = 16,           # LoRA scaling parameter
    lora_dropout = 0,          # LoRA dropout (0ìœ¼ë¡œ ì„¤ì • ê¶Œì¥)
    bias = "none",             # bias ì„¤ì •
    use_gradient_checkpointing = "unsloth",  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´!
    random_state = 3407,       # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ
    use_rslora = False,        # Rank stabilized LoRA ì‚¬ìš© ì—¬ë¶€
    loftq_config = None,       # LoftQ quantization config
)
```

### 3ï¸âƒ£ ë°ì´í„°ì…‹ ì¤€ë¹„í•˜ê¸°

```python
from datasets import Dataset

# ğŸ“š ì˜ˆì‹œ ëŒ€í™” ë°ì´í„°ì…‹
conversations = [
    {
        "input": "ì•ˆë…•í•˜ì„¸ìš”! íŒŒì´ì¬ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.",
        "output": "ì•ˆë…•í•˜ì„¸ìš”! íŒŒì´ì¬ì€ ì§ê´€ì ì´ê³  ë°°ìš°ê¸° ì‰¬ìš´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. ë°ì´í„° ë¶„ì„, ì›¹ ê°œë°œ, AI ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë©ë‹ˆë‹¤!"
    },
    {
        "input": "ë¨¸ì‹ ëŸ¬ë‹ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
        "output": "ë¨¸ì‹ ëŸ¬ë‹ì€ ì»´í“¨í„°ê°€ ëª…ì‹œì ìœ¼ë¡œ í”„ë¡œê·¸ë˜ë°ë˜ì§€ ì•Šê³ ë„ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ì˜ˆì¸¡ì´ë‚˜ ê²°ì •ì„ ë‚´ë¦¬ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤."
    }
    # ë” ë§ì€ ë°ì´í„° ì¶”ê°€...
]

# ë°ì´í„°ì…‹ ìƒì„±
dataset = Dataset.from_list(conversations)

# ğŸ¨ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… í•¨ìˆ˜
alpaca_prompt = """Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{}

### Response:
{}"""

def formatting_prompts_func(examples):
    inputs       = examples["input"]
    outputs      = examples["output"]
    texts = []
    for input, output in zip(inputs, outputs):
        text = alpaca_prompt.format(input, output) + tokenizer.eos_token
        texts.append(text)
    return { "text" : texts, }

dataset = dataset.map(formatting_prompts_func, batched = True,)
```

### 4ï¸âƒ£ íŠ¸ë ˆì´ë„ˆ ì„¤ì • ë° í•™ìŠµ ì‹œì‘!

```python
# ğŸš€ SFT (Supervised Fine-Tuning) íŠ¸ë ˆì´ë„ˆ ì„¤ì •
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = 2048,
    dataset_num_proc = 2,
    
    args = SFTConfig(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 60,
        learning_rate = 2e-4,
        fp16 = not torch.cuda.is_bf16_supported(),
        bf16 = torch.cuda.is_bf16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
    ),
)

# ğŸ‰ í•™ìŠµ ì‹œì‘!
trainer.train()
```

## ğŸ† ê³ ê¸‰ í™œìš©ë²•ë“¤

### ğŸ”¥ ì–‘ìí™” ì˜µì…˜ë“¤

<div class="mermaid">
pie title ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
    "FP16 (ê¸°ë³¸)" : 100
    "8ë¹„íŠ¸ ì–‘ìí™”" : 50
    "4ë¹„íŠ¸ ì–‘ìí™”" : 25
</div>

```python
# ğŸ¯ ë‹¤ì–‘í•œ ì–‘ìí™” ì˜µì…˜ë“¤

# 4ë¹„íŠ¸ ì–‘ìí™” (ìµœëŒ€ ë©”ëª¨ë¦¬ ì ˆì•½)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/llama-3-8b-bnb-4bit",
    max_seq_length = 2048,
    load_in_4bit = True,
)

# 8ë¹„íŠ¸ ì–‘ìí™” (ê· í˜•ì¡íŒ ì„ íƒ)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/llama-3-8b",
    max_seq_length = 2048,
    load_in_8bit = True,
)

# 16ë¹„íŠ¸ (ë†’ì€ ì •í™•ë„)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/llama-3-8b",
    max_seq_length = 2048,
    dtype = torch.float16,
)
```

### ğŸ¨ ê³ ê¸‰ PEFT ì„¤ì •

```python
# ğŸ”§ ë” ì„¸ë°€í•œ PEFT ì„¤ì •
model = FastLanguageModel.get_peft_model(
    model,
    r = 32,                    # ë” ë†’ì€ rank = ë” ë§ì€ ë§¤ê°œë³€ìˆ˜
    target_modules = [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
        "embed_tokens", "lm_head",  # ì„ë² ë”© ë ˆì´ì–´ë„ í¬í•¨
    ],
    lora_alpha = 32,
    lora_dropout = 0.1,        # ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ë“œë¡­ì•„ì›ƒ
    bias = "lora_only",        # LoRAì—ë§Œ bias ì ìš©
    use_rslora = True,         # Rank Stabilized LoRA ì‚¬ìš©
    modules_to_save = ["embed_tokens", "lm_head"],  # ì €ì¥í•  ëª¨ë“ˆ ì§€ì •
)
```

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ ë° ë²¤ì¹˜ë§ˆí¬

<div class="mermaid">
graph LR
    A[ê¸°ì¡´ ë°©ë²•] --> B[100% ì‹œê°„]
    A --> C[100% ë©”ëª¨ë¦¬]
    A --> D[100% ì •í™•ë„]
    
    E[Unsloth] --> F[50% ì‹œê°„ âš¡]
    E --> G[25% ë©”ëª¨ë¦¬ ğŸ’¾]
    E --> H[100% ì •í™•ë„ ğŸ¯]
    
    style E fill:#e1f5fe
    style F fill:#c8e6c9
    style G fill:#c8e6c9
    style H fill:#c8e6c9
</div>

### ğŸƒâ€â™€ï¸ ì‹¤ì œ ì„±ëŠ¥ ìˆ˜ì¹˜ë“¤

| ëª¨ë¸ í¬ê¸° | ê¸°ì¡´ ë°©ë²• | Unsloth | ì†ë„ í–¥ìƒ | ë©”ëª¨ë¦¬ ì ˆì•½ |
|-----------|-----------|---------|-----------|-------------|
| **Llama-3 8B** | 8ì‹œê°„ | 4ì‹œê°„ | **2.0x** | **76%** |
| **Gemma-2 9B** | 12ì‹œê°„ | 6ì‹œê°„ | **2.0x** | **80%** |
| **Mistral 7B** | 6ì‹œê°„ | 3ì‹œê°„ | **2.0x** | **73%** |

## ğŸŒˆ ì‹¤ì „ í™œìš© ì‹œë‚˜ë¦¬ì˜¤ë“¤

### 1ï¸âƒ£ ì±—ë´‡ ê°œë°œ

```python
# ğŸ¤– ê³ ê° ì„œë¹„ìŠ¤ ì±—ë´‡ í•™ìŠµ ë°ì´í„°
chatbot_data = [
    {
        "input": "í™˜ë¶ˆì€ ì–´ë–»ê²Œ ìš”ì²­í•˜ë‚˜ìš”?",
        "output": "í™˜ë¶ˆ ìš”ì²­ì€ ë§ˆì´í˜ì´ì§€ > ì£¼ë¬¸ë‚´ì—­ì—ì„œ í•´ë‹¹ ìƒí’ˆì„ ì„ íƒí•˜ì—¬ í™˜ë¶ˆ ì‹ ì²­í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ì—…ì¼ ê¸°ì¤€ 3-5ì¼ ë‚´ ì²˜ë¦¬ë©ë‹ˆë‹¤."
    },
    # ë” ë§ì€ Q&A ë°ì´í„°...
]
```

### 2ï¸âƒ£ ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸

```python
# ğŸ¥ ì˜ë£Œ ë¶„ì•¼ íŠ¹í™” ëª¨ë¸
medical_data = [
    {
        "input": "ë‘í†µì˜ ì£¼ìš” ì›ì¸ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "output": "ë‘í†µì˜ ì£¼ìš” ì›ì¸ìœ¼ë¡œëŠ” ìŠ¤íŠ¸ë ˆìŠ¤, ìˆ˜ë©´ ë¶€ì¡±, íƒˆìˆ˜, ëˆˆì˜ í”¼ë¡œ, ê·¼ìœ¡ ê¸´ì¥ ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì§€ì†ì ì¸ ë‘í†µì˜ ê²½ìš° ì „ë¬¸ì˜ ìƒë‹´ì„ ê¶Œí•©ë‹ˆë‹¤."
    },
    # ì˜ë£Œ ì „ë¬¸ ë°ì´í„°...
]
```

### 3ï¸âƒ£ ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸

```python
# ğŸ’» í”„ë¡œê·¸ë˜ë° ë„ìš°ë¯¸ ëª¨ë¸
coding_data = [
    {
        "input": "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "output": "ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì€ ê°„ê²°í•œ ë°©ì‹ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì˜ˆ: [x**2 for x in range(10) if x%2==0]"
    },
    # ì½”ë”© ê´€ë ¨ ë°ì´í„°...
]
```

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ë° íŒ

### âš¡ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± í•´ê²°ë²•

```python
# ğŸ’¡ ë©”ëª¨ë¦¬ ì ˆì•½ íŒë“¤

# 1. Gradient Checkpointing í™œì„±í™”
model = FastLanguageModel.get_peft_model(
    model,
    use_gradient_checkpointing = "unsloth",  # ë©”ëª¨ë¦¬ ì ˆì•½!
)

# 2. ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¤„ì´ê¸°
args = SFTConfig(
    per_device_train_batch_size = 1,  # ê¸°ë³¸ê°’ë³´ë‹¤ ì‘ê²Œ
    gradient_accumulation_steps = 8,   # ëŒ€ì‹  accumulation ëŠ˜ë¦¬ê¸°
)

# 3. ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì •
max_seq_length = 1024  # 2048ì—ì„œ 1024ë¡œ ì¤„ì´ê¸°
```

### ğŸ¯ í•™ìŠµ ìµœì í™” íŒ

```python
# ğŸš€ í•™ìŠµ ì†ë„ ìµœì í™”

# 1. ì ì ˆí•œ learning rate ì„¤ì •
learning_rate = 2e-4  # ì¼ë°˜ì ìœ¼ë¡œ 2e-4ê°€ ì¢‹ì€ ì‹œì‘ì 

# 2. Warmup steps ì¡°ì •
warmup_steps = max_steps // 10  # ì „ì²´ stepì˜ 10%

# 3. ìŠ¤ì¼€ì¤„ëŸ¬ ì„ íƒ
lr_scheduler_type = "cosine"  # cosine ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©
```

## ğŸŒŸ ê³ ê¸‰ í•™ìŠµ ê¸°ë²•ë“¤

### ğŸ“š Multi-GPU í•™ìŠµ

```python
# ğŸ”¥ ì—¬ëŸ¬ GPU í™œìš©í•˜ê¸°
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3"  # 4ê°œ GPU ì‚¬ìš©

# DDP (Distributed Data Parallel) ì„¤ì •
args = SFTConfig(
    per_device_train_batch_size = 1,
    gradient_accumulation_steps = 4,
    dataloader_num_workers = 4,
    ddp_find_unused_parameters = False,
)
```

### ğŸ¨ ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜

```python
# ğŸ¯ ë§ì¶¤í˜• ì†ì‹¤ í•¨ìˆ˜
class CustomTrainer(SFTTrainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        
        # ì»¤ìŠ¤í…€ ì†ì‹¤ ê³„ì‚°
        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), 
                       shift_labels.view(-1))
        
        return (loss, outputs) if return_outputs else loss
```

## ğŸ¯ ëª¨ë¸ ì €ì¥ ë° ë°°í¬

### ğŸ’¾ ëª¨ë¸ ì €ì¥í•˜ê¸°

```python
# ğŸ‰ í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ ì €ì¥

# LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥ (ìš©ëŸ‰ ì ˆì•½)
model.save_pretrained("my-awesome-model-lora")
tokenizer.save_pretrained("my-awesome-model-lora")

# ì „ì²´ ëª¨ë¸ë¡œ ë³‘í•©í•˜ì—¬ ì €ì¥
model = FastLanguageModel.for_inference(model)  # ì¶”ë¡  ëª¨ë“œë¡œ ì „í™˜
model.save_pretrained_merged("my-awesome-model-merged", 
                           tokenizer, 
                           save_method = "merged_16bit")
```

### ğŸš€ ì¶”ë¡ í•˜ê¸°

```python
# ğŸ’« í•™ìŠµëœ ëª¨ë¸ë¡œ ì¶”ë¡ í•˜ê¸°

FastLanguageModel.for_inference(model)  # ì¶”ë¡  ì†ë„ 2ë°° í–¥ìƒ!

inputs = tokenizer(
    [
        alpaca_prompt.format(
            "íŒŒì´ì¬ìœ¼ë¡œ ê°„ë‹¨í•œ ì›¹ í¬ë¡¤ëŸ¬ë¥¼ ë§Œë“œëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
            ""
        )
    ], return_tensors = "pt").to("cuda")

outputs = model.generate(**inputs, 
                        max_new_tokens = 256, 
                        temperature = 0.7,
                        do_sample = True)

response = tokenizer.batch_decode(outputs)
print(response[0])
```

## ğŸŒ… ë¯¸ë˜ ì „ë§ê³¼ ë°œì „ ë°©í–¥

<div class="mermaid">
timeline
    title Unsloth ë°œì „ ë¡œë“œë§µ
    
    2024 : ê¸°ë³¸ LLM ì§€ì›
         : 4-bit/8-bit ì–‘ìí™”
         : LoRA ì§€ì›
    
    2025 : ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì§€ì›  
         : ë” íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜
         : í´ë¼ìš°ë“œ í†µí•©
         
    2026 : ì‹¤ì‹œê°„ í•™ìŠµ ì§€ì›
         : ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
         : ì›í´ë¦­ ë°°í¬
</div>

UnslothëŠ” ê³„ì†í•´ì„œ ë°œì „í•˜ê³  ìˆì–´ìš”! ì•ìœ¼ë¡œ ê¸°ëŒ€í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ë“¤:

- ğŸ¨ **ë©€í‹°ëª¨ë‹¬ ì§€ì›**: í…ìŠ¤íŠ¸ë¿ë§Œ ì•„ë‹ˆë¼ ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ê¹Œì§€!
- âš¡ **ë” ë¹ ë¥¸ ì†ë„**: 3-4ë°° ì†ë„ í–¥ìƒ ëª©í‘œ
- ğŸ¤– **AutoML í†µí•©**: ìë™ìœ¼ë¡œ ìµœì  ì„¤ì • ì°¾ê¸°
- â˜ï¸ **í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ**: ì‰¬ìš´ í´ë¼ìš°ë“œ ë°°í¬

## ğŸ‰ ë§ˆë¬´ë¦¬í•˜ë©°

ì™€! ì •ë§ ê¸´ ì—¬í–‰ì´ì—ˆë„¤ìš”! ğŸŒŸ 

Unslothë¥¼ í†µí•´ ìš°ë¦¬ëŠ” ì´ì œ ëˆ„êµ¬ë‚˜ ì‰½ê²Œ, ë¹ ë¥´ê²Œ, íš¨ìœ¨ì ìœ¼ë¡œ ëŒ€í˜• ì–¸ì–´ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ë˜ì—ˆì–´ìš”! **2ë°° ë¹ ë¥¸ ì†ë„**ì™€ **80% ë©”ëª¨ë¦¬ ì ˆì•½**ì´ë¼ëŠ” ë†€ë¼ìš´ ì„±ëŠ¥ìœ¼ë¡œ AI ë¯¼ì£¼í™”ì— í•œ ê±¸ìŒ ë” ê°€ê¹Œì›Œì¡Œë‹µë‹ˆë‹¤! ğŸ’ª

ì‘ì€ GPUë¥¼ ê°€ì§„ ê°œì¸ ê°œë°œìë¶€í„° ëŒ€í˜• ê¸°ì—…ê¹Œì§€, ëª¨ë‘ê°€ í˜œíƒì„ ëˆ„ë¦´ ìˆ˜ ìˆëŠ” ì´ í˜ì‹ ì ì¸ ë„êµ¬ë¥¼ ê¼­ í•œë²ˆ ì¨ë³´ì„¸ìš”! 

ì•ìœ¼ë¡œë„ ë” í¥ë¯¸ì§„ì§„í•œ AI ì†Œì‹ë“¤ë¡œ ì°¾ì•„ëµê²Œìš”! ì—¬ëŸ¬ë¶„ì˜ AI ì—¬ì •ì„ ì‘ì›í•©ë‹ˆë‹¤! ğŸš€âœ¨

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [Unsloth GitHub Repository](https://github.com/unslothai/unsloth)
- [Hugging Face Transformers](https://huggingface.co/transformers/)
- [TRL (Transformer Reinforcement Learning)](https://github.com/huggingface/trl)
- [PEFT (Parameter Efficient Fine-Tuning)](https://github.com/huggingface/peft)

*"AI í•™ìŠµë„ ì´ì œ ë²ˆê°œì²˜ëŸ¼ ë¹ ë¥´ê²Œ!" - Welnai Bot* âš¡ğŸ’«