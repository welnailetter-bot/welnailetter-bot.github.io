---
date: 2025-08-04 12:00:00
layout: post
title: "AIê°€ ë‚´ ë§ì„ ì°°ë–¡ê°™ì´ ì•Œì•„ë“£ëŠ”ë‹¤ê³ ? OpenAI Whisper ì™„ë²½ ì •ë³µê¸°!"
subtitle: "ìŒì„±ì¸ì‹ ëíŒì™• Whisper, Aë¶€í„° Zê¹Œì§€! êµ¬ì¡°, ì„±ëŠ¥, ê¿€íŒê¹Œì§€ íƒˆíƒˆ í„¸ì–´ë“œë ¤ìš” âœ¨"
description: >-
  "ë‚´ ë§ì„ ì´ë ‡ê²Œ ì˜ ì•Œì•„ë“£ëŠ”ë‹¤ê³ ?" OpenAIì˜ ì´ˆê°•ë ¥ ìŒì„±ì¸ì‹ ëª¨ë¸ Whisperì˜ ëª¨ë“  ê²ƒì„ íŒŒí—¤ì³ë´…ë‹ˆë‹¤. 99ê°œ ì–¸ì–´ ì§€ì›, ë†€ë¼ìš´ ì •í™•ë„, ê·¸ë¦¬ê³  ë‹¹ì‹ ì˜ í”„ë¡œì íŠ¸ë¥¼ í•œ ë‹¨ê³„ ì—…ê·¸ë ˆì´ë“œ ì‹œì¼œì¤„ íŒŒì¸íŠœë‹ê³¼ ì–‘ìí™” ê¿€íŒê¹Œì§€!
image: https://placehold.co/760x399/7E57C2/FFFFFF?text=Whisper+AI
optimized_image: https://placehold.co/380x200/7E57C2/FFFFFF?text=Whisper
category: AI
tags:
  - OpenAI
  - Whisper
  - ASR
  - Speech Recognition
  - Machine Learning
  - Fine-tuning
  - Quantization
  - AI Model
author: welnai
paginate: true
mermaid: true
---

"ì•„ë‹ˆ, AIê°€ ë‚´ ë§ì„ ì´ë ‡ê²Œ ì˜ ì•Œì•„ë“£ëŠ”ë‹¤ê³ ?ğŸ˜²"

í˜¹ì‹œ ìŒì„± ë¹„ì„œí•œí…Œ "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?" ë¬¼ì–´ë´¤ë‹¤ê°€ "ì˜¤ë… ë‚ ì¹˜ì•Œ ì°œ" ê°™ì€ ë™ë¬¸ì„œë‹µì„ ë“¤ì–´ë³¸ ì  ìˆìœ¼ì‹ ê°€ìš”? (ì›ƒí”ˆ ê²½í—˜... ë‹¤ë“¤ í•œ ë²ˆì¯¤ì€ ìˆì–ì•„ìš”?ğŸ˜‚)

ì´ì œ ê·¸ëŸ° ë‹µë‹µí•¨ì€ ì•ˆë…•! 2022ë…„ 9ì›”, OpenAIê°€ ì„¸ìƒì„ ê¹œì§ ë†€ë¼ê²Œ í•œ ìŒì„± ì¸ì‹ê³„ì˜ ì–´ë²¤ì ¸ìŠ¤, **Whisper**ê°€ ë“±ì¥í–ˆê¸° ë•Œë¬¸ì´ì£ . ë¬´ë ¤ **68ë§Œ ì‹œê°„**ì´ë¼ëŠ” ì–´ë§ˆì–´ë§ˆí•œ ì–‘ì˜ ë°ì´í„°ë¡œ í›ˆë ¨í•´ì„œ, ì „ ì„¸ê³„ **99ê°œ ì–¸ì–´**ë¥¼ ê±°ì˜ ë‹¤ ì•Œì•„ë“£ëŠ” ëŠ¥ë ¥ìëë‹ˆë‹¤.

ë‹¨ìˆœíˆ ë§ì„ ê¸€ë¡œ ë°›ì•„ì“°ëŠ” ê±¸ ë„˜ì–´, ë²ˆì—­ì— ì–¸ì–´ íƒì§€ê¹Œì§€! ê·¸ì•¼ë§ë¡œ ìŒì„± ì¸ì‹ê³„ì˜ 'ë§ŒëŠ¥ ì¹˜íŠ¸í‚¤'ë¼ê³  í•  ìˆ˜ ìˆì£ . ì§€ê¸ˆë¶€í„° ì €ì™€ í•¨ê»˜ Whisperì˜ ì„¸ê³„ë¡œ ì‹ ë‚˜ê²Œ ë¹ ì ¸ë³¼ê¹Œìš”?

### ğŸ•µï¸â€â™‚ï¸ WhisperëŠ” ì–´ë–»ê²Œ ë§ì„ ì•Œì•„ë“¤ì„ê¹Œ? (feat. ë˜‘ë˜‘í•œ ë²ˆì—­ê°€)

Whisperì˜ í•µì‹¬ êµ¬ì¡°ëŠ” ë°”ë¡œ **Encoder-Decoder Transformer**ë¼ëŠ” ë©‹ì§„ ì´ë¦„ì„ ê°€ì§„ ì•„í‚¤í…ì²˜ì˜ˆìš”. ì´ë¦„ë§Œ ë“¤ìœ¼ë©´ ì–´ë µë‹¤ê³ ìš”? ì „í˜€ìš”! UNì˜ ë™ì‹œí†µì—­ì‚¬ë¥¼ ë– ì˜¬ë¦¬ë©´ ì•„ì£¼ ì‰¬ì›Œìš”.

1.  **Encoder (ì—´ì‹¬íˆ ë“£ëŠ” ê·€ğŸ‘‚)**: ë¨¼ì € 'ì¸ì½”ë”'ë¼ëŠ” ì¹œêµ¬ê°€ ì—¬ëŸ¬ë¶„ì˜ ëª©ì†Œë¦¬(ì˜¤ë””ì˜¤)ë¥¼ ê¼¼ê¼¼íˆ ë“£ê³ , ì»´í“¨í„°ê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ 'ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨'ì´ë¼ëŠ” íŠ¹ë³„í•œ ì–¸ì–´ë¡œ ìš”ì•½ ì •ë¦¬ë¥¼ í•´ìš”. ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ ì ˆëŒ€ ë†“ì¹˜ì§€ ì•ŠëŠ” ì•„ì£¼ ì˜ˆë¦¬í•œ ê·€ë¥¼ ê°€ì¡Œì£ !
2.  **Decoder (ìˆ ìˆ  ë§í•˜ëŠ” ì…ğŸ‘„)**: ê·¸ ë‹¤ìŒ, 'ë””ì½”ë”'ë¼ëŠ” ì¹œêµ¬ê°€ ì¸ì½”ë”ê°€ ì •ë¦¬í•œ ë…¸íŠ¸ë¥¼ ë°›ì•„ì„œ, ìš°ë¦¬ê°€ ì½ì„ ìˆ˜ ìˆëŠ” ì™„ë²½í•œ ë¬¸ì¥(í…ìŠ¤íŠ¸)ìœ¼ë¡œ ìˆ ìˆ  í’€ì–´ì£¼ëŠ” ì—­í• ì„ í•´ìš”.

ì´ ë‘ ì¹œêµ¬ì˜ í™˜ìƒì ì¸ íŒ€ì›Œí¬ ë•ë¶„ì—, ì–´ë–¤ ë§ì´ë“  ì°°ë–¡ê°™ì´ ì•Œì•„ë“¤ì„ ìˆ˜ ìˆëŠ” ê±°ëë‹ˆë‹¤!

![Whisperì˜ êµ¬ì¡°ë¥¼ ì„¤ëª…í•˜ëŠ” ì´ë¯¸ì§€](https://placehold.co/600x300/4DB6AC/FFFFFF?text=Encoder+%2B+Decoder+Teamwork!)

<div class="mermaid">
graph TB
    subgraph Architecture["Whisper Architecture"]
        A[Raw Audio Input] --> B[Mel Spectrogram]
        B --> C[Audio Encoder]
        C --> D[Cross Attention]
        E[Text Tokens] --> F[Text Decoder]
        D --> F
        F --> G[Output Text]
        
        subgraph Transformer["Encoder-Decoder Transformer"]
            C
            F
        end
        
        subgraph Tokens["Special Tokens"]
            H[Language Token]
            I[Task Token] 
            J[Timestamp Token]
        end
        
        H --> F
        I --> F
        J --> F
    end
    
    style A fill:#e1f5fe
    style G fill:#c8e6c9
    style C fill:#fff3e0
    style F fill:#fff3e0
</div>

ì—¬ê¸°ì— **íŠ¹ìˆ˜ í† í°**ì´ë¼ëŠ” ì¡°ë¯¸ë£Œê¹Œì§€ íŒíŒ! "ì´ê±´ í•œêµ­ì–´ì•¼!", "ì´ê±´ ë²ˆì—­í•´ì¤˜!", "ì´ ë¶€ë¶„ì€ 3ë¶„ 15ì´ˆì— í•œ ë§ì´ì•¼!" ê°™ì€ ì¶”ê°€ ì •ë³´ë¥¼ ì½•ì½• ì§‘ì–´ì£¼ë‹ˆ, ì •í™•ë„ê°€ ì•ˆ ë†’ì„ ìˆ˜ê°€ ì—†ê² ì£ ?

 

### ğŸš€ Whisper, ë­ê°€ ê·¸ë ‡ê²Œ ëŒ€ë‹¨í•œë°? (ìŠ¤í™ ìë‘ íƒ€ì„)

#### ğŸŒ 99ê°œ ì–¸ì–´ ì§€ì›! ì´ê²Œ ë˜ë„¤?

ì˜ì–´ëŠ” ê¸°ë³¸, í•œêµ­ì–´, ì¼ë³¸ì–´, ìŠ¤í˜ì¸ì–´ëŠ” ë¬¼ë¡ ì´ê³  ìš°ë¦¬ê°€ ì´ë¦„ë„ ìƒì†Œí•œ ì–¸ì–´ê¹Œì§€ ì´ 99ê°œ ì–¸ì–´ë¥¼ ì§€ì›í•´ìš”. ì „ ì„¸ê³„ ëˆ„êµ¬ì™€ë„ ì†Œí†µí•  ìˆ˜ ìˆëŠ” ë§ŒëŠ¥ ë²ˆì—­ê¸°ë¥¼ ê°€ì§„ ê¸°ë¶„ì´ë„ê¹Œìš”?

<div class="mermaid">
pie title Whisper í›ˆë ¨ ë°ì´í„° ë¶„í¬ (ì‹œê°„ ê¸°ì¤€)
    "ì˜ì–´" : 438000
    "ì¤‘êµ­ì–´" : 42000
    "ë…ì¼ì–´" : 28000
    "ìŠ¤í˜ì¸ì–´" : 25000
    "í”„ë‘ìŠ¤ì–´" : 23000
    "ì¼ë³¸ì–´" : 18000
    "í•œêµ­ì–´" : 8000
    "ê¸°íƒ€ ì–¸ì–´" : 98000
</div>

#### ğŸ§  í˜¼ìì„œë„ ì˜í•´ìš”! ë©€í‹°íƒœìŠ¤í‚¹ ëŠ¥ë ¥ì

1.  **ìŒì„± ì¸ì‹ (Transcription)**: ë§í•˜ëŠ” ëŒ€ë¡œ í…ìŠ¤íŠ¸ë¡œ ì°©ì°©!
2.  **ë²ˆì—­ (Translation)**: í•œêµ­ì–´ ìŒì„±ì„ ì˜ì–´ í…ìŠ¤íŠ¸ë¡œ ë°”ë¡œ! (ë§ˆë²• ê°™ì£ ?)
3.  **ì–¸ì–´ ì‹ë³„ (Language Identification)**: "ì´ê±° ì–´ëŠ ë‚˜ë¼ ë§ì´ì§€?" ê¶ê¸ˆí•´í•  í•„ìš” ì—†ì´ Whisperê°€ ì•Œë ¤ì¤˜ìš”.
4.  **ìŒì„± í™œë™ ê°ì§€ (Voice Activity Detection)**: ë§í•˜ëŠ” êµ¬ê°„ë§Œ ë˜‘ë˜‘í•˜ê²Œ ì°¾ì•„ë‚´ìš”.

#### ğŸ’ª ì‹œë„ëŸ¬ì›Œë„ ê´œì°®ì•„! ë…¸ì´ì¦ˆ ìº”ìŠ¬ë§ê¸‰ ê°•ì¸í•¨

ì£¼ë³€ì´ ì‹œë„ëŸ¬ìš´ ì¹´í˜ì—ì„œ ë…¹ìŒí–ˆê±°ë‚˜, ë…íŠ¹í•œ ì–µì–‘ìœ¼ë¡œ ë§í•´ë„ WhisperëŠ” ë¬¸ì œ ì—†ì–´ìš”. ì›¬ë§Œí•œ ë°©í•´ë¬¼ì€ ê°€ë¿íˆ ì´ê²¨ë‚´ëŠ” íŠ¼íŠ¼í•œ ê·€ë¥¼ ê°€ì¡Œë‹µë‹ˆë‹¤.

#### âš–ï¸ ê³¨ë¼ ì“°ëŠ” ì¬ë¯¸! ëª¨ë¸ í¬ê¸°ë³„ ì„±ëŠ¥ ë¹„êµ

WhisperëŠ” tiny, base, small, medium, large ë“± ë‹¤ì–‘í•œ ì‚¬ì´ì¦ˆì˜ ëª¨ë¸ì´ ìˆì–´ìš”. ë§ˆì¹˜ ì˜· ì‚¬ì´ì¦ˆì²˜ëŸ¼, ë‚´ ìƒí™©ì— ë”± ë§ëŠ” ëª¨ë¸ì„ ê³ ë¥´ë©´ ëœë‹µë‹ˆë‹¤!

- **ì‘ì€ ëª¨ë¸ (Tiny, Base)**: ê°€ë³ê³  ë¹¨ë¼ìš”! ê°„ë‹¨í•œ ì‘ì—…ì— ë”±! âš¡ï¸
- **í° ëª¨ë¸ (Medium, Large)**: ë©ì¹˜ê°€ í° ë§Œí¼ ë” ì •í™•í•˜ê³  ë˜‘ë˜‘í•´ìš”! ìµœê³ ì˜ ì„±ëŠ¥ì„ ì›í•œë‹¤ë©´! ğŸ‘‘

![ëª¨ë¸ ì‚¬ì´ì¦ˆë¥¼ ê³ ë¥´ëŠ” ì´ë¯¸ì§€](https://placehold.co/600x300/FF7043/FFFFFF?text=Choose+Your+Fighter!)

<div class="mermaid">
graph LR
    subgraph Models["Whisper Model Variants"]
        A[Tiny<br/>39M params] --> B{Accuracy vs Speed}
        C[Base<br/>74M params] --> B
        D[Small<br/>244M params] --> B
        E[Medium<br/>769M params] --> B
        F[Large<br/>1550M params] --> B
        G[Large-v2<br/>1550M params] --> B
        H[Large-v3<br/>1550M params] --> B
        
        B --> I[Higher Accuracy<br/>Slower Speed]
        B --> J[Lower Accuracy<br/>Faster Speed]
    end
    
    style A fill:#ffcdd2
    style C fill:#f8bbd9
    style D fill:#e1bee7
    style E fill:#c5cae9
    style F fill:#bbdefb
    style G fill:#b2dfdb
    style H fill:#c8e6c9
</div>



### ğŸ‡°ğŸ‡· í•œêµ­ì–´ ì„±ëŠ¥, ì†”ì§íˆ ì–´ë–¤ë°?

"ê·¸ë˜ì„œ... í•œêµ­ì–´ëŠ” ì˜ ì•Œì•„ë“¤ì–´?" ê°€ì¥ ê¶ê¸ˆí•œ ë¶€ë¶„ì´ì£ ! WhisperëŠ” ë¬´ë ¤ **8,000ì‹œê°„**ì˜ í•œêµ­ì–´ ë°ì´í„°ë¡œ ê³µë¶€í–ˆì–´ìš”. ì „ì²´ 68ë§Œ ì‹œê°„ ì¤‘ì—ì„œëŠ” ì‘ì€ ë¹„ì¤‘ì´ì§€ë§Œ, ì˜ì–´ ë‹¤ìŒìœ¼ë¡œ 7ë²ˆì§¸ë¡œ ë§ì€ ì–‘ì´ë‹ˆ í•œêµ­ì–´ ì„±ëŠ¥ë„ ìƒë‹¹íˆ ë›°ì–´ë‚˜ë‹µë‹ˆë‹¤!

#### ì‚´ì§ ì•„ì‰¬ìš´ ì ì€? (feat. ê·€ì—¬ìš´ ì‹¤ìˆ˜)

ë¬¼ë¡  ì•„ì§ ì™„ë²½í•˜ì§„ ì•Šì•„ìš”. ê°€ë” **ë„ì–´ì“°ê¸°**ë¥¼ ê¹œë¹¡í•˜ê±°ë‚˜, **ë¬¸ì¥ ë¶€í˜¸**ë¥¼ ë¹¼ë¨¹ëŠ” ê·€ì—¬ìš´ ì‹¤ìˆ˜ë¥¼ í•˜ê¸°ë„ í•´ìš”. "ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤" ê°™ì€ ëŠë‚Œì´ë„ê¹Œìš”? ğŸ˜‚ ê·¸ë˜ì„œ ì™„ë²½í•œ ê¸€ì„ ì›í•œë‹¤ë©´ ì‚´ì§ ì†ë´ì£¼ëŠ” í›„ì²˜ë¦¬ ê³¼ì •ì´ í•„ìš”í•˜ë‹µë‹ˆë‹¤.


### ğŸ“ ë‚´ ì…ë§›ëŒ€ë¡œ íŠœë‹í•˜ê¸°! íŒŒì¸íŠœë‹(Fine-tuning)

WhisperëŠ” ì´ë¯¸ ë˜‘ë˜‘í•œ ë§ŒëŠ¥ ì¸ì¬ì§€ë§Œ, íŠ¹ì • ë¶„ì•¼ì˜ 'ì „ë¬¸ê°€'ë¡œ ë§Œë“¤ ìˆ˜ë„ ìˆì–´ìš”. ì´ê²Œ ë°”ë¡œ **íŒŒì¸íŠœë‹**ì…ë‹ˆë‹¤!

ì˜ˆë¥¼ ë“¤ì–´, ì˜í•™ ìš©ì–´ê°€ ê°€ë“í•œ íšŒì˜ë¥¼ ë…¹ìŒí•´ì•¼ í•œë‹¤ë©´? ê¸°ë³¸ WhisperëŠ” "í—¤ëª¨ê¸€ë¡œë¹ˆ" ê°™ì€ ë‹¨ì–´ë¥¼ ì˜ ëª¨ë¥¼ ìˆ˜ ìˆê² ì£ . ì´ë•Œ ì˜í•™ ì „ë¬¸ ìš©ì–´ ë°ì´í„°ë¥¼ ì¶”ê°€ë¡œ í•™ìŠµì‹œì¼œì£¼ë©´, WhisperëŠ” ìˆœì‹ê°„ì— ì˜ë£Œ ì „ë¬¸ ë¹„ì„œë¡œ ë³€ì‹ í•©ë‹ˆë‹¤! ğŸ‘¨â€âš•ï¸

ì‹¬ì§€ì–´ **í•­ê³µ ê´€ì œ ìŒì„±**ì´ë‚˜ **íŠ¹ì • ì§€ì—­ ì‚¬íˆ¬ë¦¬**ì²˜ëŸ¼ ë§¤ìš° íŠ¹ìˆ˜í•œ ë¶„ì•¼ì—ì„œë„ íŒŒì¸íŠœë‹ì´ ê°€ëŠ¥í•´ìš”. ì •ë§ ëŒ€ë‹¨í•˜ì§€ ì•Šë‚˜ìš”?

![íŒŒì¸íŠœë‹ìœ¼ë¡œ ì „ë¬¸ê°€ê°€ ë˜ëŠ” Whisper](https://placehold.co/600x300/5C6BC0/FFFFFF?text=Whisper+Gets+a+PhD!)

#### ğŸ¯ LoRAë¡œ ë” ìŠ¤ë§ˆíŠ¸í•˜ê²Œ! íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  íŒŒì¸íŠœë‹

ìµœê·¼ì—” **LoRA (Low-Rank Adaptation)**ë¼ëŠ” ë˜‘ë˜‘í•œ ë°©ë²•ë„ ìˆì–´ìš”! ì „ì²´ ëª¨ë¸ì„ ë‹¤ì‹œ í›ˆë ¨í•˜ëŠ” ëŒ€ì‹ , ê¼­ í•„ìš”í•œ ë¶€ë¶„ë§Œ ì‚´ì§ ì¡°ì •í•´ì„œ ì‹œê°„ê³¼ ë¹„ìš©ì„ í™• ì¤„ì´ëŠ” ê¸°ìˆ ì´ì£ . ë§ˆì¹˜ ì˜·ì˜ í•ì„ ì‚´ì§ë§Œ ìˆ˜ì„ í•˜ëŠ” ê²ƒì²˜ëŸ¼ìš”! âœ‚ï¸

#### íŒŒì¸íŠœë‹, ëŒ€ì¶© ì´ëŸ° ëŠë‚Œ!

<div class="mermaid">
graph TB
    subgraph BaseModel["ê¸°ë³¸ Whisper ëª¨ë¸"]
        A[Base Model<br/>68ë§Œ ì‹œê°„ í•™ìŠµ ì™„ë£Œ]
    end
    
    subgraph DomainData["ì „ë¬¸ ë¶„ì•¼ ë°ì´í„°"]
        B[êµ¬ìŒ ì¥ì•  í™˜ì ìŒì„±]
        C[ì˜ë£Œ ë¶„ì•¼ ìŒì„±]
        D[íŠ¹ì • ì§€ì—­ ì‚¬íˆ¬ë¦¬]
        E[í•­ê³µ ê´€ì œ ìŒì„±]
        K[ê²Œì„ ìš©ì–´ ë°ì´í„°]
    end
    
    subgraph FineTuning["íŒŒì¸íŠœë‹ í”„ë¡œì„¸ìŠ¤"]
        F[ë°ì´í„° ì „ì²˜ë¦¬]
        G[íŠ¹ì§• ì¶”ì¶œ]
        H[ëª¨ë¸ í›ˆë ¨]
        I[ì„±ëŠ¥ í‰ê°€]
    end
    
    subgraph OptimizedModel["ìµœì í™”ëœ ëª¨ë¸"]
        J[ë„ë©”ì¸ íŠ¹í™”<br/>Whisper ëª¨ë¸]
    end
    
    A --> F
    B --> F
    C --> F
    D --> F
    E --> F
    K --> F
    
    F --> G
    G --> H
    H --> I
    I --> J
    
    style A fill:#e3f2fd
    style J fill:#e8f5e8
    style H fill:#fff3e0
</div>

#### ì½”ë“œë¡œ ì‚´ì§ ì—¿ë³´ê¸° (HuggingFace)

"ì½”ë“œëŠ”... ì–´ë ¤ìš´ë°...?" ê±±ì • ë§ˆì„¸ìš”! HuggingFace ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ë©´ ìƒê°ë³´ë‹¤ ê°„ë‹¨í•˜ê²Œ íŒŒì¸íŠœë‹ì„ í•  ìˆ˜ ìˆì–´ìš”.

<details markdown="1">
<summary>ğŸ‘† HuggingFace ê¸°ë³¸ íŒŒì¸íŠœë‹ ì½”ë“œ ë³´ê¸° (í´ë¦­í•´ì„œ í¼ì¹˜ê¸°! ğŸ¤–)</summary>

```python
from transformers import (
    WhisperFeatureExtractor,
    WhisperTokenizer,
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainer
)

# 1. í•„ìš”í•œ ì¬ë£Œë“¤ì„ ë¶ˆëŸ¬ì™€ìš” (ëª¨ë¸, í† í¬ë‚˜ì´ì € ë“±)
feature_extractor = WhisperFeatureExtractor.from_pretrained("openai/whisper-small")
tokenizer = WhisperTokenizer.from_pretrained("openai/whisper-small")
processor = WhisperProcessor.from_pretrained("openai/whisper-small")
model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")

# 2. í›ˆë ¨ ì‹œì‘! (ë°ì´í„°ë¥¼ ë„£ê³  í›ˆë ¨ ë²„íŠ¼ë§Œ ëˆ„ë¥´ë©´ ë!)
trainer = Seq2SeqTrainer(
    model=model,
    # ... (í›ˆë ¨ ì„¤ì •ì€ ì—¬ê¸°ì—)
)

trainer.train() # "ê³µë¶€ ì‹œì‘!"
```

</details>

#### ğŸ¯ LoRA íŒŒì¸íŠœë‹ ì‹¤ì „ ì½”ë“œ! (ë” ìŠ¤ë§ˆíŠ¸í•˜ê²Œ íŠœë‹í•˜ê¸°)

"ì „ì²´ ëª¨ë¸ì„ ë‹¤ì‹œ í›ˆë ¨í•˜ê¸°ì—” ë„ˆë¬´ ë¬´ê²ê³ ... ğŸ’¸ ë¹„ìš©ë„ ë¶€ë‹´ìŠ¤ëŸ½ê³ ..." ê·¸ëŸ° ë¶„ë“¤ì„ ìœ„í•œ **LoRA íŒŒì¸íŠœë‹** ì½”ë“œì˜ˆìš”! 

<details markdown="1">
<summary>ğŸ¯ LoRA íŒŒì¸íŠœë‹ ì™„ì „ ì½”ë“œ ë³´ê¸° (0.21% íŒŒë¼ë¯¸í„°ë§Œ í›ˆë ¨í•˜ëŠ” ë§ˆë²•! âœ¨)</summary>

```python
from peft import LoraConfig, get_peft_model, TaskType
from transformers import (
    WhisperForConditionalGeneration, 
    WhisperProcessor,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments
)
import torch

# 1. ê¸°ë³¸ Whisper ëª¨ë¸ ë¡œë“œ ğŸ¤–
model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
processor = WhisperProcessor.from_pretrained("openai/whisper-small")

# 2. LoRA ì„¤ì • (ì—¬ê¸°ê°€ í•µì‹¬! âœ¨)
lora_config = LoraConfig(
    r=32,  # ë­í¬ í¬ê¸° (ë†’ì„ìˆ˜ë¡ ë” ì •êµí•˜ì§€ë§Œ ë¬´ê±°ì›Œì§)
    lora_alpha=64,  # LoRA ìŠ¤ì¼€ì¼ë§ íŒ©í„°
    target_modules=["q_proj", "v_proj"],  # ì–´ëŠ ë¶€ë¶„ì„ íŠœë‹í• ì§€ ì„ íƒ
    lora_dropout=0.1,  # ê³¼ì í•© ë°©ì§€
    bias="none",
    task_type=TaskType.FEATURE_EXTRACTION,
)

# 3. LoRA ëª¨ë¸ë¡œ ë³€ì‹ ! ğŸ¦¸â€â™‚ï¸
peft_model = get_peft_model(model, lora_config)

# 4. í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë§Œ í™•ì¸í•´ë³´ê¸° (ë†€ë¼ì‹¤ ê±°ì˜ˆìš”!)
peft_model.print_trainable_parameters()
# ê²°ê³¼: "ì „ì²´ 741,875,712ê°œ ì¤‘ì—ì„œ trainable: 1,572,864 (0.21%ë§Œ í›ˆë ¨!) ğŸ‰"

# 5. í›ˆë ¨ ì„¤ì •
training_args = Seq2SeqTrainingArguments(
    output_dir="./whisper-lora-finetuned",
    per_device_train_batch_size=16,
    gradient_accumulation_steps=1,
    warmup_steps=50,
    max_steps=1000,  # LoRAëŠ” ë¹ ë¥´ê²Œ í•™ìŠµë˜ë‹ˆê¹Œ!
    learning_rate=1e-3,  # ê¸°ë³¸ë³´ë‹¤ ì¡°ê¸ˆ ë†’ê²Œ
    fp16=True,  # ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œ
    evaluation_strategy="steps",
    per_device_eval_batch_size=8,
    predict_with_generate=True,
    generation_max_length=225,
    save_steps=100,
    eval_steps=100,
    logging_steps=25,
    report_to=["tensorboard"],  # í•™ìŠµ ê³¼ì • ì‹œê°í™”
    load_best_model_at_end=True,
    metric_for_best_model="wer",  # WER(Word Error Rate)ë¡œ ìµœê³  ëª¨ë¸ ì„ íƒ
    greater_is_better=False,
    push_to_hub=False,  # í—ˆê¹…í˜ì´ìŠ¤ì— ì—…ë¡œë“œí• ì§€ ì—¬ë¶€
)

# 6. íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í›ˆë ¨ ì‹œì‘! ğŸš€
trainer = Seq2SeqTrainer(
    model=peft_model,
    args=training_args,
    train_dataset=train_dataset,  # ì—¬ëŸ¬ë¶„ì˜ í›ˆë ¨ ë°ì´í„°
    eval_dataset=eval_dataset,    # ê²€ì¦ ë°ì´í„°
    tokenizer=processor.feature_extractor,
    data_collator=data_collator,
)

# 7. ë“œë””ì–´ í›ˆë ¨! (ì‹œê°„ê³¼ ë©”ëª¨ë¦¬ê°€ í™• ì¤„ì–´ë“  ê±¸ ëŠë¼ì‹¤ ê±°ì˜ˆìš”!)
trainer.train()

# 8. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
peft_model.save_pretrained("./best-whisper-lora")
print("LoRA íŒŒì¸íŠœë‹ ì™„ë£Œ! ğŸ‰ ì´ì œ ì—¬ëŸ¬ë¶„ë§Œì˜ ì „ë¬¸ Whisperê°€ íƒ„ìƒí–ˆì–´ìš”!")
```

</details>

#### ğŸ’¡ LoRA íŒŒì¸íŠœë‹ì˜ ë§ˆë²•! 

<details markdown="1">
<summary>ğŸ’« í•™ìŠµëœ LoRA ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì½”ë“œ (ë‚˜ì¤‘ì— ì¨ë¨¹ì„ ë•Œ! ğŸš€)</summary>

```python
# ì €ì¥ëœ LoRA ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (ë‚˜ì¤‘ì— ì‚¬ìš©í•  ë•Œ)
from peft import PeftModel

base_model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
lora_model = PeftModel.from_pretrained(base_model, "./best-whisper-lora")

# ì´ì œ ì—¬ëŸ¬ë¶„ë§Œì˜ ì „ë¬¸ Whisper ì‚¬ìš© ì¤€ë¹„ ì™„ë£Œ! âœ¨
```

</details>



### ğŸ‹ï¸â€â™€ï¸ ëª¨ë¸ ë‹¤ì´ì–´íŠ¸ ì‹œí‚¤ê¸°! ì–‘ìí™”(Quantization)

"Large ëª¨ë¸ ì“°ê³  ì‹¶ì€ë°... ë„ˆë¬´ ë¬´ê²ê³  ëŠë ¤ìš” ğŸ˜­"

ê±±ì • ë§ˆì„¸ìš”! ìš°ë¦¬ì—ê² **ì–‘ìí™”**ë¼ëŠ” ë‹¤ì´ì–´íŠ¸ ë¹„ë²•ì´ ìˆìŠµë‹ˆë‹¤. ì–‘ìí™”ëŠ” ëª¨ë¸ì˜ í’ˆì§ˆì€ ìµœëŒ€í•œ ìœ ì§€í•˜ë©´ì„œ, ìš©ëŸ‰ì€ í™• ì¤„ì´ê³  ì†ë„ëŠ” ìŒ©ìŒ©í•˜ê²Œ ë§Œë“œëŠ” ê¸°ìˆ ì´ì—ìš”.

ê³ í™”ì§ˆ ì›ë³¸ ì˜ìƒ(FP32)ì„ í™”ì§ˆ ì €í•˜ ê±°ì˜ ì—†ì´ ìš©ëŸ‰ë§Œ ì¤„ì¸ ì••ì¶• íŒŒì¼(INT8)ë¡œ ë§Œë“œëŠ” ê²ƒê³¼ ê°™ì•„ìš”. ë•ë¶„ì— ë‚´ ë…¸íŠ¸ë¶ì€ ë¬¼ë¡ ì´ê³ , **ìŠ¤ë§ˆíŠ¸í°ì´ë‚˜ ë¼ì¦ˆë² ë¦¬íŒŒì´** ê°™ì€ ì‘ì€ ê¸°ê¸°ì—ì„œë„ Whisperë¥¼ ëŒë¦´ ìˆ˜ ìˆê²Œ ë˜ì£ ! ğŸ“±

![ì–‘ìí™”ë¡œ ê°€ë²¼ì›Œì§€ëŠ” ëª¨ë¸](https://placehold.co/600x300/26A69A/FFFFFF?text=Model+Diet+Plan!)

#### ì–‘ìí™” íš¨ê³¼, ì´ ì •ë„ë¼ê³ ?

<div class="mermaid">
graph TD
    subgraph Comparison["ì–‘ìí™” ì „í›„ ë¹„êµ"]
        A[ì›ë³¸ ëª¨ë¸ FP32] --> B{ì–‘ìí™” ì²˜ë¦¬}
        B --> C[INT8 ëª¨ë¸]
        B --> D[INT4 ëª¨ë¸]
        B --> E[INT16 ëª¨ë¸]
        
        subgraph Performance["ì„±ëŠ¥ ê°œì„ "]
            F[ëª¨ë¸ í¬ê¸° ìµœëŒ€ 75% ê°ì†Œ]
            G[ì¶”ë¡  ì†ë„ 4-20ë°° í–¥ìƒ]
            H[ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 64% ì ˆì•½]
            I[ì§€ì—° ì‹œê°„ 19% ë‹¨ì¶•]
            J[ì—£ì§€ ê¸°ê¸° ë°°í¬ ê°€ëŠ¥]
        end
        
        C --> F
        C --> G
        D --> F
        D --> H
        E --> I
        C --> J
        D --> J
    end
    
    style A fill:#ffcdd2
    style C fill:#c8e6c9
    style D fill:#a5d6a7
    style E fill:#dcedc1
</div>

#### ì–‘ìí™”, ë­˜ë¡œ í•˜ì§€? (ì¸ê¸° ë¼ì´ë¸ŒëŸ¬ë¦¬)

1.  **HuggingFace Optimum + CTranslate2**: ê°€ì¥ ëŒ€ì¤‘ì ì´ê³  ì“°ê¸° í¸í•œ ì¡°í•©!
2.  **faster-whisper**: ì†ë„ì— ë¯¸ì¹œë“¯ì´ ì§‘ì°©í•˜ê³  ì‹¶ì„ ë•Œ! âš¡ï¸
3.  **whisper.cpp**: C++ ê¸°ë°˜ìœ¼ë¡œ, ê·¹í•œì˜ ìµœì í™”ë¥¼ ì›í•  ë•Œ!
4.  **GPTQ**: ì •í™•ë„ ì†ì‹¤ì€ ìµœì†Œí™”í•˜ë©´ì„œ, íŒíŒ ì••ì¶•í•˜ê³  ì‹¶ì„ ë•Œ!

#### ğŸš€ faster-whisperë¡œ ë²ˆê°œ ì²˜ë¦¬! (ì œì¼ ì‰¬ìš´ ë°©ë²•)

"ë³µì¡í•œ ê±´ ì‹«ê³ , ê·¸ëƒ¥ ë¹¨ë¼ì§€ë©´ ì¢‹ê² ì–´!" í•˜ëŠ” ë¶„ë“¤ì„ ìœ„í•œ **faster-whisper** ì‚¬ìš©ë²•ì´ì—ìš”!

<details markdown="1">
<summary>âš¡ faster-whisper ì‚¬ìš©ë²• ë³´ê¸° (4-20ë°° ë¹¨ë¼ì§€ëŠ” ë§ˆë²•! ğŸš„)</summary>

```python
from faster_whisper import WhisperModel
import time

# 1. ëª¨ë¸ ë¡œë“œ (INT8 ì–‘ìí™” ìë™ ì ìš©! âœ¨)
model = WhisperModel("large-v3", device="cuda", compute_type="int8")
# CPU ì‚¬ìš©ì‹œ: device="cpu", compute_type="int8"
# ë” ì••ì¶•: compute_type="int4"  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë”ìš± ì¤„ì´ê¸°!

# 2. ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ (ê¸°ë³¸ ì²˜ë¦¬ì™€ ë˜‘ê°™ì•„ìš”!)
start_time = time.time()
segments, info = model.transcribe("audio.mp3", language="ko")

# 3. ê²°ê³¼ í™•ì¸
print(f"ì–¸ì–´: {info.language} (í™•ì‹ ë„: {info.language_probability:.2f})")
print(f"ì´ ê¸¸ì´: {info.duration:.2f}ì´ˆ")

# 4. ì„¸ê·¸ë¨¼íŠ¸ë³„ë¡œ í…ìŠ¤íŠ¸ ì¶œë ¥ (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨!)
for segment in segments:
    print(f"[{segment.start:.2f}s â†’ {segment.end:.2f}s] {segment.text}")
    
end_time = time.time()
print(f"ì²˜ë¦¬ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ - ë¹¨ë¼ì§„ ê±° ëŠê»´ì§€ì‹œë‚˜ìš”? ğŸš„")
```

</details>

#### âš¡ HuggingFace Optimumìœ¼ë¡œ INT8 ì••ì¶•í•˜ê¸°

"í—ˆê¹…í˜ì´ìŠ¤ ìƒíƒœê³„ë¥¼ ë²—ì–´ë‚˜ê¸° ì‹«ì–´!" í•˜ëŠ” ë¶„ë“¤ì„ ìœ„í•œ ë°©ë²•ì´ì—ìš”!

<details markdown="1">
<summary>ğŸ¤— HuggingFace Optimumìœ¼ë¡œ INT8 ì••ì¶•í•˜ê¸° (BetterTransformer + ì–‘ìí™”! ğŸš€)</summary>

```python
from optimum.bettertransformer import BetterTransformer
from transformers import WhisperForConditionalGeneration, WhisperProcessor
import torch

# 1. ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
model = WhisperForConditionalGeneration.from_pretrained(
    "openai/whisper-small",
    torch_dtype=torch.float16  # ì ˆë°˜ ì •ë°€ë„ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
)
processor = WhisperProcessor.from_pretrained("openai/whisper-small")

# 2. BetterTransformerë¡œ ìµœì í™” (ì†ë„ í–¥ìƒ! ğŸš€)
model = BetterTransformer.transform(model)

# 3. PyTorch 2.0ì˜ ì»´íŒŒì¼ ê¸°ëŠ¥ìœ¼ë¡œ ë”ìš± ë¹ ë¥´ê²Œ!
if hasattr(torch, 'compile'):
    model.generate = torch.compile(model.generate, mode="reduce-overhead")
    print("ëª¨ë¸ ì»´íŒŒì¼ ì™„ë£Œ! ë”ìš± ë¹¨ë¼ì¡Œì–´ìš”! âš¡")

# 4. INT8 ì–‘ìí™” (ë™ì  ì–‘ìí™”)
model = torch.quantization.quantize_dynamic(
    model, 
    {torch.nn.Linear},  # Linear ë ˆì´ì–´ë§Œ ì–‘ìí™”
    dtype=torch.qint8
)

print("ì–‘ìí™” ì™„ë£Œ! ëª¨ë¸ì´ ë‹¤ì´ì–´íŠ¸ì— ì„±ê³µí–ˆì–´ìš”! ğŸ‰")
```

</details>

#### ğŸ”¥ whisper.cppë¡œ ê·¹í•œ ìµœì í™”! (ê³ ê¸‰ììš©)

"ë‚´ ë¼ì¦ˆë² ë¦¬íŒŒì´ì—ì„œë„ ëŒë¦¬ê³  ì‹¶ì–´!" í•˜ëŠ” ìš©ê°í•œ ë¶„ë“¤ì„ ìœ„í•œ ë°©ë²•ì´ì—ìš”!

<details markdown="1">
<summary>ğŸ”¥ whisper.cpp ì„¤ì¹˜ & ì‚¬ìš©ë²• ë³´ê¸° (ë¼ì¦ˆë² ë¦¬íŒŒì´ì—ì„œë„ OK! ğŸ¥§)</summary>

```bash
# 1. ì„¤ì¹˜ (Linux/Mac)
git clone https://github.com/ggerganov/whisper.cpp.git
cd whisper.cpp
make

# 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜
bash ./models/download-ggml-model.sh base
# ë˜ëŠ” ë” ì••ì¶•ëœ ì–‘ìí™” ë²„ì „
bash ./models/download-ggml-model.sh base.en-q5_0  # Q5 ì–‘ìí™” ë²„ì „

# 3. ì‹¤í–‰! (ëª…ë ¹ì–´ í•˜ë‚˜ë¡œ ë!)
./main -m models/ggml-base.bin -f audio.wav -l ko

# 4. Pythonì—ì„œ ì‚¬ìš©í•˜ê¸°
```

</details>

<details markdown="1">
<summary>ğŸ Pythonì—ì„œ whisper.cpp í˜¸ì¶œí•˜ê¸° (subprocess í™œìš©! ğŸ”§)</summary>

```python
import subprocess
import os

def transcribe_with_whisper_cpp(audio_file, model_path="models/ggml-base.bin", language="ko"):
    """
    whisper.cppë¥¼ Pythonì—ì„œ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜
    """
    cmd = [
        "./whisper.cpp/main",
        "-m", model_path,
        "-f", audio_file,
        "-l", language,
        "--output-txt"  # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
    ]
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            print("whisper.cpp ì²˜ë¦¬ ì™„ë£Œ! ğŸ‰")
            # ê²°ê³¼ íŒŒì¼ ì½ê¸°
            txt_file = audio_file.replace('.wav', '.txt')
            if os.path.exists(txt_file):
                with open(txt_file, 'r', encoding='utf-8') as f:
                    return f.read()
        else:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {result.stderr}")
            return None
    except Exception as e:
        print(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

# ì‚¬ìš© ì˜ˆì œ
result = transcribe_with_whisper_cpp("my_audio.wav")
print(f"ê²°ê³¼: {result}")
```

</details>

#### ğŸ’ ì„±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí¬ ì½”ë“œ

"ì§„ì§œë¡œ ë¹¨ë¼ì¡ŒëŠ”ì§€ í™•ì¸í•´ë³´ê³  ì‹¶ì–´!" í•˜ëŠ” ë¶„ë“¤ì„ ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ ì½”ë“œì˜ˆìš”!

<details markdown="1">
<summary>ğŸ’ ì„±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí¬ ì½”ë“œ ë³´ê¸° (ì–¼ë§ˆë‚˜ ë¹¨ë¼ì¡ŒëŠ”ì§€ ì§ì ‘ í™•ì¸! ğŸ“Š)</summary>

```python
import time
import psutil
import torch
from faster_whisper import WhisperModel as FastWhisperModel
from transformers import WhisperForConditionalGeneration, WhisperProcessor

def benchmark_whisper_models(audio_file="test_audio.mp3"):
    """
    ì—¬ëŸ¬ Whisper êµ¬í˜„ì²´ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” í•¨ìˆ˜
    """
    results = {}
    
    # 1. ì›ë³¸ HuggingFace Whisper
    print("ğŸŒ ì›ë³¸ HuggingFace Whisper í…ŒìŠ¤íŠ¸ ì¤‘...")
    start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
    start_time = time.time()
    
    model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
    processor = WhisperProcessor.from_pretrained("openai/whisper-small")
    # ì—¬ê¸°ì„œ ì‹¤ì œ ì²˜ë¦¬... (ì½”ë“œ ìƒëµ)
    
    end_time = time.time()
    end_memory = psutil.Process().memory_info().rss / 1024 / 1024
    
    results['original'] = {
        'time': end_time - start_time,
        'memory': end_memory - start_memory,
        'model_size': 'Large (1.5GB)'
    }
    
    # 2. faster-whisper
    print("ğŸš„ faster-whisper í…ŒìŠ¤íŠ¸ ì¤‘...")
    start_memory = psutil.Process().memory_info().rss / 1024 / 1024
    start_time = time.time()
    
    fast_model = FastWhisperModel("small", device="cpu", compute_type="int8")
    segments, info = fast_model.transcribe(audio_file, language="ko")
    list(segments)  # ì‹¤ì œë¡œ ì²˜ë¦¬í•˜ê¸°
    
    end_time = time.time()
    end_memory = psutil.Process().memory_info().rss / 1024 / 1024
    
    results['faster_whisper'] = {
        'time': end_time - start_time,
        'memory': end_memory - start_memory,
        'model_size': 'Compressed (~400MB)'
    }
    
    # 3. ê²°ê³¼ ì¶œë ¥
    print("\nğŸ† ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:")
    for name, result in results.items():
        print(f"{name}:")
        print(f"  â±ï¸  ì²˜ë¦¬ ì‹œê°„: {result['time']:.2f}ì´ˆ")
        print(f"  ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©: {result['memory']:.1f}MB")
        print(f"  ğŸ“¦ ëª¨ë¸ í¬ê¸°: {result['model_size']}")
        print()
    
    # ì†ë„ ê°œì„  ê³„ì‚°
    if len(results) >= 2:
        original_time = results['original']['time']
        fast_time = results['faster_whisper']['time']
        speedup = original_time / fast_time
        print(f"ğŸš€ faster-whisperê°€ {speedup:.1f}ë°° ë” ë¹¨ë¼ìš”! ëŒ€ë°•ì´ì£ ? ğŸ‰")

# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
benchmark_whisper_models()
```

</details>

#### ë°©ë²•ë³„ ì„±ëŠ¥ ë¹„êµ (ê³¨ë¼ë´!)

<div class="mermaid">
graph LR
    subgraph Methods["ì–‘ìí™” ë°©ë²•ë³„ ì„±ëŠ¥ ë¹„êµ"]
        A[FP32 ì›ë³¸] 
        B[FP16 ì ˆë°˜ ì••ì¶•]
        C[INT8 1/4 ì••ì¶•]
        D[INT4 1/8 ì••ì¶•]
        
        A --> B
        B --> C
        C --> D
        
        subgraph Note["ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„"]
            E[ì••ì¶•ë¥  ì¦ê°€ ì‹œ<br/>ì†ë„ í–¥ìƒ<br/>ì •í™•ë„ ê°ì†Œ ê°€ëŠ¥]
        end
        
        D --> E
    end
    
    style A fill:#ffcdd2
    style B fill:#f8bbd9
    style C fill:#c5cae9
    style D fill:#a5d6a7
    style E fill:#fff3e0
</div>



### ğŸ¤© ê·¸ë˜ì„œ ì´ê±¸ë¡œ ë­˜ í•  ìˆ˜ ìˆëŠ”ë°? (í™œìš© ì‚¬ë¡€ ë¬´í•œëŒ€)

WhisperëŠ” ìƒìƒí•˜ëŠ” ëª¨ë“  ê²ƒì„ í˜„ì‹¤ë¡œ ë§Œë“¤ì–´ ì¤„ ìˆ˜ ìˆì–´ìš”!

<div class="mermaid">
mindmap
  root((Whisper í™œìš©ë²•))
    ì½˜í…ì¸  ì œì‘
      ìœ íŠœë¸Œ ìë™ ìë§‰
      íŒŸìºìŠ¤íŠ¸ ëŒ€ë³¸
      ê°•ì˜ ë‚´ìš© ê¸°ë¡
      ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° ìë§‰
    ìŠ¤ë§ˆíŠ¸ ì—…ë¬´
      íšŒì˜ë¡ ì‘ì„± ìë™í™”
      ì½œì„¼í„° í†µí™” ë¶„ì„
      ìŒì„± ëª…ë ¹ ë¹„ì„œ
      í•­ê³µ ê´€ì œ í†µì‹ 
    ì˜ë£Œ ë¶„ì•¼
      ì§„ë£Œ ê¸°ë¡
      ì‘ê¸‰ ìƒí™© ë¶„ì„
      ìˆ˜ìˆ ì‹¤ ìŒì„± ê¸°ë¡
    ì ‘ê·¼ì„± ê°œì„ 
      ì‹¤ì‹œê°„ ìë§‰
      ì™¸êµ­ì–´ ì˜ìƒ ë²ˆì—­
      ì²­ê° ì¥ì• ì¸ ì§€ì›
    ì—£ì§€ ë””ë°”ì´ìŠ¤
      IoT ê¸°ê¸° ìŒì„± ì¸ì‹
      ìŠ¤ë§ˆíŠ¸í° ì˜¤í”„ë¼ì¸ ì²˜ë¦¬
      ì„ë² ë””ë“œ ì‹œìŠ¤í…œ
    ê°œë°œì ë„êµ¬
      ìŒì„± ì œì–´ ì•±
      AI ì±—ë´‡ ìŒì„± ì¸ì‹
      ê²Œì„ ë‚´ ìŒì„± ëª…ë ¹
</div>

### ğŸ’» ë‚˜ë„ ì¨ë³´ì! Whisper API ì‚¬ìš©ë²•

"ì™€, ë‹¹ì¥ ì¨ë³´ê³  ì‹¶ì–´!" í•˜ëŠ” ë¶„ë“¤ì„ ìœ„í•´ ì¤€ë¹„í–ˆì–´ìš”. ì½”ë”©ì„ ì˜ ëª°ë¼ë„ ê´œì°®ì•„ìš”. OpenAI APIë¥¼ ì‚¬ìš©í•˜ë©´ ì •ë§ ê°„ë‹¨í•˜ê²Œ Whisperë¥¼ ì´ìš©í•  ìˆ˜ ìˆë‹µë‹ˆë‹¤.

#### íŒŒì´ì¬ìœ¼ë¡œ Whisper ë§›ë³´ê¸°

<details  markdown="1">
<summary>ğŸ¯ OpenAI APIë¡œ ê°„ë‹¨í•˜ê²Œ ì¨ë³´ê¸° (ì½”ë”© ì´ˆë³´ë„ OK! ğŸ‘¶)</summary>

```python
from openai import OpenAI

client = OpenAI()

# ë‚´ ëª©ì†Œë¦¬ íŒŒì¼(audio.mp3)ì„ í…ìŠ¤íŠ¸ë¡œ ë°”ê¿”ì¤˜! (í•œêµ­ì–´ë¡œ)
with open("audio.mp3", "rb") as audio_file:
    transcription = client.audio.transcriptions.create(
        model="whisper-1",
        file=audio_file,
        language="ko"  # í•œêµ­ì–´ë¼ê³  ì•Œë ¤ì£¼ë©´ ë” ì˜ ì•Œì•„ë“¤ì–´ìš”!
    )
    print(transcription.text)

# í•œêµ­ì–´ ì˜¤ë””ì˜¤ íŒŒì¼(korean_audio.mp3)ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ì¤˜!
with open("korean_audio.mp3", "rb") as audio_file:
    translation = client.audio.translations.create(
        model="whisper-1",
        file=audio_file
    )
    print(translation.text)
```

</details>

#### ì§€ì›í•˜ëŠ” ì˜¤ë””ì˜¤ í˜•ì‹

-   **íŒŒì¼ í˜•ì‹**: mp3, mp4, mpeg, mpga, m4a, wav, webm (ì›¬ë§Œí•œ ê±´ ë‹¤ ë¼ìš”!)
-   **íŒŒì¼ í¬ê¸°**: ìµœëŒ€ 25MB (ë„ˆë¬´ ê¸´ íŒŒì¼ì€ ì˜ë¼ì„œ ë„£ì–´ì£¼ì„¸ìš”!)

### ğŸ¬ ì‹¤ì „ í™œìš©! ì‹œë‚˜ë¦¬ì˜¤ë³„ ì½”ë“œ ì˜ˆì œì§‘

"ì´ë¡ ì€ ì•Œê² ëŠ”ë°... ì‹¤ì œë¡œëŠ” ì–´ë–»ê²Œ ì¨ì•¼ í• ê¹Œìš”?" ê¶ê¸ˆí•œ ë¶„ë“¤ì„ ìœ„í•œ **ì‹¤ì „ ì½”ë“œ ëª¨ìŒì§‘**ì´ì—ìš”!

#### ğŸ“º ì‹¤ì‹œê°„ ë§ˆì´í¬ ì…ë ¥ ì²˜ë¦¬ (ë¼ì´ë¸Œ ìë§‰!)

<details markdown="1">

<summary>"ì‹¤ì‹œê°„ìœ¼ë¡œ ë‚´ ëª©ì†Œë¦¬ë¥¼ ìë§‰ìœ¼ë¡œ ë§Œë“¤ê³  ì‹¶ì–´!" í•˜ëŠ” ë¶„ë“¤ì„ ìœ„í•œ ì½”ë“œì˜ˆìš”!</summary>

```python
import pyaudio
import wave
import tempfile
import threading
import time
from faster_whisper import WhisperModel
from collections import deque

class RealTimeWhisper:
    def __init__(self, model_size="base", language="ko"):
        # Whisper ëª¨ë¸ ë¡œë“œ
        self.model = WhisperModel(model_size, device="cuda", compute_type="int8")
        self.language = language
        
        # ì˜¤ë””ì˜¤ ì„¤ì •
        self.CHUNK = 1024
        self.FORMAT = pyaudio.paInt16
        self.CHANNELS = 1
        self.RATE = 16000
        self.RECORD_SECONDS = 3  # 3ì´ˆì”© ì˜ë¼ì„œ ì²˜ë¦¬
        
        # PyAudio ì´ˆê¸°í™”
        self.audio = pyaudio.PyAudio()
        self.is_recording = False
        self.transcript_queue = deque(maxlen=10)  # ìµœê·¼ 10ê°œ ê²°ê³¼ ë³´ê´€
        
    def record_audio_chunk(self):
        """3ì´ˆ ë™ì•ˆ ì˜¤ë””ì˜¤ ë…¹ìŒ"""
        stream = self.audio.open(
            format=self.FORMAT,
            channels=self.CHANNELS,
            rate=self.RATE,
            input=True,
            frames_per_buffer=self.CHUNK
        )
        
        frames = []
        for _ in range(0, int(self.RATE / self.CHUNK * self.RECORD_SECONDS)):
            if not self.is_recording:
                break
            data = stream.read(self.CHUNK)
            frames.append(data)
        
        stream.stop_stream()
        stream.close()
        
        return frames
    
    def save_temp_audio(self, frames):
        """ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ë¡œ ì €ì¥"""
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
        
        wf = wave.open(temp_file.name, 'wb')
        wf.setnchannels(self.CHANNELS)
        wf.setsampwidth(self.audio.get_sample_size(self.FORMAT))
        wf.setframerate(self.RATE)
        wf.writeframes(b''.join(frames))
        wf.close()
        
        return temp_file.name
    
    def transcribe_chunk(self, audio_file):
        """ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        try:
            segments, info = self.model.transcribe(audio_file, language=self.language)
            text = " ".join([segment.text for segment in segments])
            return text.strip()
        except Exception as e:
            print(f"ë³€í™˜ ì˜¤ë¥˜: {e}")
            return ""
    
    def start_real_time_transcription(self):
        """ì‹¤ì‹œê°„ ì „ì‚¬ ì‹œì‘!"""
        print("ğŸ¤ ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì‹œì‘! (Ctrl+Cë¡œ ì¢…ë£Œ)")
        print("=" * 50)
        
        self.is_recording = True
        
        try:
            while self.is_recording:
                # 1. ì˜¤ë””ì˜¤ ë…¹ìŒ
                print("ğŸ”´ ë…¹ìŒ ì¤‘... (3ì´ˆ)")
                frames = self.record_audio_chunk()
                
                if not frames:
                    continue
                
                # 2. ì„ì‹œ íŒŒì¼ ì €ì¥
                temp_file = self.save_temp_audio(frames)
                
                # 3. í…ìŠ¤íŠ¸ ë³€í™˜ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì²˜ë¦¬)
                print("âš¡ ì²˜ë¦¬ ì¤‘...")
                text = self.transcribe_chunk(temp_file)
                
                # 4. ê²°ê³¼ ì¶œë ¥
                if text:
                    timestamp = time.strftime("%H:%M:%S")
                    print(f"[{timestamp}] {text}")
                    self.transcript_queue.append((timestamp, text))
                else:
                    print("ğŸ”‡ (ìŒì„±ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤)")
                
                # 5. ì„ì‹œ íŒŒì¼ ì •ë¦¬
                import os
                os.unlink(temp_file)
                
        except KeyboardInterrupt:
            print("\nâœ‹ ì‹¤ì‹œê°„ ì „ì‚¬ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            self.stop()
    
    def stop(self):
        """ë…¹ìŒ ì¤‘ì§€"""
        self.is_recording = False
        self.audio.terminate()
        
        # ì „ì²´ ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“ ì „ì²´ ëŒ€í™” ë‚´ìš©:")
        print("-" * 50)
        for timestamp, text in self.transcript_queue:
            print(f"[{timestamp}] {text}")

# ì‚¬ìš©ë²•
if __name__ == "__main__":
    transcriber = RealTimeWhisper(model_size="small", language="ko")
    transcriber.start_real_time_transcription()
```
</details>

#### ğŸµ ëŒ€ìš©ëŸ‰ ì˜¤ë””ì˜¤ íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬ (ìœ íŠœë¸Œ ì˜ìƒë“¤ í•œë²ˆì—!)

<details  markdown="1">
<summary>"ìœ íŠœë¸Œ ì˜ìƒ 100ê°œë¥¼ í•œë²ˆì— ìë§‰ìœ¼ë¡œ ë§Œë“¤ê³  ì‹¶ì–´!" í•˜ëŠ” ì•¼ì‹¬ì°¬ ë¶„ë“¤ì„ ìœ„í•œ ì½”ë“œì˜ˆìš”!</summary>

```python
import os
import glob
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from faster_whisper import WhisperModel
import json
from datetime import datetime
import logging

class BatchWhisperProcessor:
    def __init__(self, model_size="large-v3", num_workers=4):
        # ê° ì›Œì»¤ë§ˆë‹¤ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì£¼ì˜!)
        self.model_size = model_size
        self.num_workers = num_workers
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('whisper_batch.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def process_single_file(self, audio_file_path, output_dir="output"):
        """ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬"""
        try:
            # ì›Œì»¤ë³„ë¡œ ëª¨ë¸ ë¡œë“œ (ìŠ¤ë ˆë“œ ì•ˆì „ì„±)
            model = WhisperModel(self.model_size, device="cuda", compute_type="int8")
            
            file_name = Path(audio_file_path).stem
            self.logger.info(f"ğŸµ ì²˜ë¦¬ ì‹œì‘: {file_name}")
            
            start_time = datetime.now()
            
            # ì „ì‚¬ ì‹¤í–‰
            segments, info = model.transcribe(
                audio_file_path,
                language="ko",
                word_timestamps=True,  # ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„
                vad_filter=True,       # VAD í•„í„° ì ìš© (ë¬´ìŒ êµ¬ê°„ ì œê±°)
                vad_parameters=dict(min_silence_duration_ms=1000)
            )
            
            # ê²°ê³¼ ì •ë¦¬
            transcript = []
            full_text = ""
            
            for segment in segments:
                segment_data = {
                    "start": segment.start,
                    "end": segment.end,
                    "text": segment.text,
                    "words": []
                }
                
                # ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ (ìˆëŠ” ê²½ìš°)
                if hasattr(segment, 'words') and segment.words:
                    for word in segment.words:
                        segment_data["words"].append({
                            "start": word.start,
                            "end": word.end,
                            "word": word.word,
                            "probability": word.probability
                        })
                
                transcript.append(segment_data)
                full_text += segment.text + " "
            
            # ê²°ê³¼ ì €ì¥
            os.makedirs(output_dir, exist_ok=True)
            
            # 1. JSON í˜•íƒœë¡œ ìƒì„¸ ê²°ê³¼ ì €ì¥
            result_data = {
                "file_name": file_name,
                "language": info.language,
                "language_probability": info.language_probability,
                "duration": info.duration,
                "processing_time": (datetime.now() - start_time).total_seconds(),
                "full_text": full_text.strip(),
                "segments": transcript
            }
            
            json_path = os.path.join(output_dir, f"{file_name}.json")
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, ensure_ascii=False, indent=2)
            
            # 2. ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥
            txt_path = os.path.join(output_dir, f"{file_name}.txt")
            with open(txt_path, 'w', encoding='utf-8') as f:
                f.write(full_text.strip())
            
            # 3. SRT ìë§‰ íŒŒì¼ ìƒì„±
            srt_path = os.path.join(output_dir, f"{file_name}.srt")
            self.create_srt_file(transcript, srt_path)
            
            processing_time = (datetime.now() - start_time).total_seconds()
            self.logger.info(f"âœ… ì™„ë£Œ: {file_name} ({processing_time:.2f}ì´ˆ)")
            
            return {
                "file": file_name,
                "status": "success",
                "duration": info.duration,
                "processing_time": processing_time,
                "language": info.language
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì˜¤ë¥˜ ({file_name}): {str(e)}")
            return {
                "file": file_name,
                "status": "error",
                "error": str(e)
            }
    
    def create_srt_file(self, segments, output_path):
        """SRT ìë§‰ íŒŒì¼ ìƒì„±"""
        with open(output_path, 'w', encoding='utf-8') as f:
            for i, segment in enumerate(segments, 1):
                start_time = self.format_time_for_srt(segment["start"])
                end_time = self.format_time_for_srt(segment["end"])
                
                f.write(f"{i}\n")
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"{segment['text'].strip()}\n\n")
    
    def format_time_for_srt(self, seconds):
        """SRT ì‹œê°„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millisecs = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"
    
    def process_directory(self, input_dir, output_dir="output", file_pattern="*.mp3"):
        """ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬"""
        # ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
        audio_files = glob.glob(os.path.join(input_dir, file_pattern))
        audio_files.extend(glob.glob(os.path.join(input_dir, "*.wav")))
        audio_files.extend(glob.glob(os.path.join(input_dir, "*.m4a")))
        audio_files.extend(glob.glob(os.path.join(input_dir, "*.mp4")))
        
        if not audio_files:
            self.logger.warning(f"ğŸ“ {input_dir}ì—ì„œ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        self.logger.info(f"ğŸ¯ ì²˜ë¦¬í•  íŒŒì¼: {len(audio_files)}ê°œ")
        
        # ë³‘ë ¬ ì²˜ë¦¬
        results = []
        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
            # ì‘ì—… ì œì¶œ
            future_to_file = {
                executor.submit(self.process_single_file, audio_file, output_dir): audio_file 
                for audio_file in audio_files
            }
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_file):
                result = future.result()
                results.append(result)
                
                # ì§„í–‰ë¥  ì¶œë ¥
                completed = len(results)
                total = len(audio_files)
                progress = (completed / total) * 100
                print(f"ğŸ“Š ì§„í–‰ë¥ : {completed}/{total} ({progress:.1f}%)")
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        self.print_summary(results)
        
        # ê²°ê³¼ í†µê³„ë¥¼ JSONìœ¼ë¡œ ì €ì¥
        summary_path = os.path.join(output_dir, "batch_summary.json")
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump({
                "total_files": len(audio_files),
                "successful": len([r for r in results if r["status"] == "success"]),
                "failed": len([r for r in results if r["status"] == "error"]),
                "results": results,
                "timestamp": datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
    
    def print_summary(self, results):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        successful = [r for r in results if r["status"] == "success"]
        failed = [r for r in results if r["status"] == "error"]
        
        print("\n" + "="*60)
        print("ğŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
        print("="*60)
        print(f"âœ… ì„±ê³µ: {len(successful)}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {len(failed)}ê°œ")
        
        if successful:
            total_duration = sum(r.get("duration", 0) for r in successful)
            total_processing_time = sum(r.get("processing_time", 0) for r in successful)
            print(f"ğŸ“Š ì´ ì˜¤ë””ì˜¤ ê¸¸ì´: {total_duration:.1f}ì´ˆ ({total_duration/60:.1f}ë¶„)")
            print(f"â±ï¸  ì´ ì²˜ë¦¬ ì‹œê°„: {total_processing_time:.1f}ì´ˆ")
            print(f"ğŸš€ ì²˜ë¦¬ ì†ë„: {total_duration/total_processing_time:.1f}x ì‹¤ì‹œê°„")
        
        if failed:
            print("\nâŒ ì‹¤íŒ¨í•œ íŒŒì¼ë“¤:")
            for result in failed:
                print(f"  - {result['file']}: {result['error']}")

# ì‚¬ìš©ë²•
if __name__ == "__main__":
    processor = BatchWhisperProcessor(
        model_size="large-v3", 
        num_workers=2  # GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì •
    )
    
    # ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬
    processor.process_directory(
        input_dir="./audio_files",      # ì˜¤ë””ì˜¤ íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë”
        output_dir="./transcriptions",  # ê²°ê³¼ê°€ ì €ì¥ë  í´ë”
        file_pattern="*.mp3"           # ì²˜ë¦¬í•  íŒŒì¼ íŒ¨í„´
    )
```

</details>

#### ğŸŒ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ í†µí•© (Flask + Whisper)

<details markdown="1">
<summary>"ë‚´ ì›¹ì‚¬ì´íŠ¸ì— ìŒì„± ì¸ì‹ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ê³  ì‹¶ì–´!" í•˜ëŠ” ê°œë°œìë¶„ë“¤ì„ ìœ„í•œ ì½”ë“œì˜ˆìš”!</summary>

```python
from flask import Flask, request, jsonify, render_template, send_file
from werkzeug.utils import secure_filename
import os
import tempfile
import threading
from faster_whisper import WhisperModel
from datetime import datetime
import uuid
import json

app = Flask(__name__)
app.config['MAX_CONTENT_LENGTH'] = 25 * 1024 * 1024  # 25MB ì œí•œ

# ì „ì—­ ì„¤ì •
UPLOAD_FOLDER = 'uploads'
ALLOWED_EXTENSIONS = {'wav', 'mp3', 'mp4', 'm4a', 'webm'}
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

# Whisper ëª¨ë¸ ë¡œë“œ (ì„œë²„ ì‹œì‘ì‹œ í•œë²ˆë§Œ)
print("ğŸ¤– Whisper ëª¨ë¸ ë¡œë”© ì¤‘...")
whisper_model = WhisperModel("base", device="cuda", compute_type="int8")
print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

# ì‘ì—… ìƒíƒœ ì¶”ì 
job_status = {}

def allowed_file(filename):
    """í—ˆìš©ëœ íŒŒì¼ í˜•ì‹ì¸ì§€ í™•ì¸"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def process_audio_async(job_id, file_path, language="auto"):
    """ë¹„ë™ê¸°ë¡œ ì˜¤ë””ì˜¤ ì²˜ë¦¬"""
    try:
        job_status[job_id] = {"status": "processing", "progress": 0}
        
        # ì „ì‚¬ ì²˜ë¦¬
        segments, info = whisper_model.transcribe(
            file_path, 
            language=None if language == "auto" else language,
            word_timestamps=True
        )
        
        # ê²°ê³¼ ì •ë¦¬
        result = {
            "language": info.language,
            "language_probability": info.language_probability,
            "duration": info.duration,
            "segments": [],
            "full_text": ""
        }
        
        full_text = ""
        for segment in segments:
            segment_data = {
                "start": segment.start,
                "end": segment.end,
                "text": segment.text
            }
            result["segments"].append(segment_data)
            full_text += segment.text + " "
        
        result["full_text"] = full_text.strip()
        
        # ì‘ì—… ì™„ë£Œ
        job_status[job_id] = {
            "status": "completed",
            "progress": 100,
            "result": result,
            "completed_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        job_status[job_id] = {
            "status": "error",
            "error": str(e),
            "failed_at": datetime.now().isoformat()
        }
    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if os.path.exists(file_path):
            os.remove(file_path)

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    return render_template('index.html')

@app.route('/upload', methods=['POST'])
def upload_file():
    """íŒŒì¼ ì—…ë¡œë“œ ë° ì²˜ë¦¬ ì‹œì‘"""
    if 'file' not in request.files:
        return jsonify({"error": "íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤"}), 400
    
    file = request.files['file']
    language = request.form.get('language', 'auto')
    
    if file.filename == '':
        return jsonify({"error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}), 400
    
    if file and allowed_file(file.filename):
        # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
        filename = secure_filename(file.filename)
        job_id = str(uuid.uuid4())
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(filename)[1])
        file.save(temp_file.name)
        
        # ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œì‘
        thread = threading.Thread(
            target=process_audio_async,
            args=(job_id, temp_file.name, language)
        )
        thread.start()
        
        return jsonify({
            "job_id": job_id,
            "message": "ì²˜ë¦¬ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”!",
            "status": "processing"
        })
    
    return jsonify({"error": "ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤"}), 400

@app.route('/status/<job_id>')
def get_status(job_id):
    """ì‘ì—… ìƒíƒœ í™•ì¸"""
    if job_id not in job_status:
        return jsonify({"error": "ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}), 404
    
    return jsonify(job_status[job_id])

@app.route('/download/<job_id>/<format>')
def download_result(job_id, format):
    """ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (txt, json, srt)"""
    if job_id not in job_status or job_status[job_id]["status"] != "completed":
        return jsonify({"error": "ì™„ë£Œëœ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤"}), 404
    
    result = job_status[job_id]["result"]
    
    if format == "txt":
        # í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
        temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt', encoding='utf-8')
        temp_file.write(result["full_text"])
        temp_file.close()
        
        return send_file(
            temp_file.name,
            as_attachment=True,
            download_name=f"transcription_{job_id}.txt",
            mimetype="text/plain"
        )
    
    elif format == "json":
        # JSON íŒŒì¼ ìƒì„±
        temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json', encoding='utf-8')
        json.dump(result, temp_file, ensure_ascii=False, indent=2)
        temp_file.close()
        
        return send_file(
            temp_file.name,
            as_attachment=True,
            download_name=f"transcription_{job_id}.json",
            mimetype="application/json"
        )
    
    elif format == "srt":
        # SRT ìë§‰ íŒŒì¼ ìƒì„±
        temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.srt', encoding='utf-8')
        
        for i, segment in enumerate(result["segments"], 1):
            start_time = format_time_for_srt(segment["start"])
            end_time = format_time_for_srt(segment["end"])
            
            temp_file.write(f"{i}\n")
            temp_file.write(f"{start_time} --> {end_time}\n")
            temp_file.write(f"{segment['text'].strip()}\n\n")
        
        temp_file.close()
        
        return send_file(
            temp_file.name,
            as_attachment=True,
            download_name=f"subtitles_{job_id}.srt",
            mimetype="text/plain"
        )
    
    return jsonify({"error": "ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹ì…ë‹ˆë‹¤"}), 400

def format_time_for_srt(seconds):
    """SRT ì‹œê°„ í˜•ì‹ ë³€í™˜"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millisecs = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"

if __name__ == '__main__':
    print("ğŸŒ Whisper Web Server ì‹œì‘!")
    print("ğŸ“ http://localhost:5000 ì—ì„œ ì ‘ì†í•˜ì„¸ìš”!")
    app.run(host='0.0.0.0', port=5000, debug=True)
```
</details>

#### ğŸ“± WebRTC ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ (ë¸Œë¼ìš°ì €ì—ì„œ!)

<details markdown="1">
<summary>"ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ ë°”ë¡œ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŒì„± ì¸ì‹í•˜ê³  ì‹¶ì–´!" í•˜ëŠ” ë¶„ë“¤ì„ ìœ„í•œ HTML + JavaScript ì½”ë“œì˜ˆìš”!</summary>

```html
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ğŸ¤ ì‹¤ì‹œê°„ Whisper ìŒì„± ì¸ì‹</title>
    <style>
        body { font-family: 'Segoe UI', sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }
        .container { background: #f5f5f5; padding: 30px; border-radius: 15px; }
        .status { padding: 15px; margin: 10px 0; border-radius: 10px; }
        .recording { background: #ffcccb; border-left: 5px solid #ff4444; }
        .processing { background: #fff4cd; border-left: 5px solid #ffaa00; }
        .completed { background: #d4edda; border-left: 5px solid #28a745; }
        button { padding: 15px 30px; font-size: 16px; border: none; border-radius: 10px; cursor: pointer; margin: 10px; }
        .start-btn { background: #28a745; color: white; }
        .stop-btn { background: #dc3545; color: white; }
        .transcript { background: white; padding: 20px; margin: 10px 0; border-radius: 10px; min-height: 100px; border: 2px dashed #ddd; }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ¤ ì‹¤ì‹œê°„ Whisper ìŒì„± ì¸ì‹</h1>
        <div id="status" class="status">ğŸ¯ ì¤€ë¹„ ì™„ë£Œ! ì‹œì‘ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.</div>
        
        <button id="startBtn" class="start-btn" onclick="startRecording()">ğŸ™ï¸ ë…¹ìŒ ì‹œì‘</button>
        <button id="stopBtn" class="stop-btn" onclick="stopRecording()" disabled>â¹ï¸ ë…¹ìŒ ì¤‘ì§€</button>
        
        <div class="transcript">
            <h3>ğŸ“ ì‹¤ì‹œê°„ ì „ì‚¬ ê²°ê³¼:</h3>
            <div id="transcript"></div>
        </div>
    </div>

    <script>
        let mediaRecorder;
        let audioChunks = [];
        let isRecording = false;
        let recordingInterval;

        async function startRecording() {
            try {
                // ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œ ìš”ì²­
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        sampleRate: 16000,
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true
                    } 
                });

                mediaRecorder = new MediaRecorder(stream, {
                    mimeType: 'audio/webm'
                });

                mediaRecorder.ondataavailable = event => {
                    audioChunks.push(event.data);
                };

                mediaRecorder.onstop = async () => {
                    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                    await sendAudioToServer(audioBlob);
                    audioChunks = [];
                };

                // UI ìƒíƒœ ë³€ê²½
                document.getElementById('startBtn').disabled = true;
                document.getElementById('stopBtn').disabled = false;
                document.getElementById('status').className = 'status recording';
                document.getElementById('status').textContent = 'ğŸ”´ ë…¹ìŒ ì¤‘... ìì—°ìŠ¤ëŸ½ê²Œ ë§ì”€í•˜ì„¸ìš”!';

                isRecording = true;

                // 3ì´ˆë§ˆë‹¤ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
                recordingInterval = setInterval(() => {
                    if (isRecording) {
                        mediaRecorder.stop();
                        mediaRecorder.start();
                    }
                }, 3000);

                mediaRecorder.start();
            } catch (error) {
                console.error('ë§ˆì´í¬ ì ‘ê·¼ ì˜¤ë¥˜:', error);
                alert('ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤!');
            }
        }

        function stopRecording() {
            if (mediaRecorder && isRecording) {
                isRecording = false;
                clearInterval(recordingInterval);
                mediaRecorder.stop();
                
                // ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ ì¤‘ì§€
                const tracks = mediaRecorder.stream.getTracks();
                tracks.forEach(track => track.stop());

                // UI ìƒíƒœ ë³€ê²½
                document.getElementById('startBtn').disabled = false;
                document.getElementById('stopBtn').disabled = true;
                document.getElementById('status').className = 'status completed';
                document.getElementById('status').textContent = 'âœ… ë…¹ìŒ ì™„ë£Œ! ë‹¤ì‹œ ì‹œì‘í•˜ë ¤ë©´ ì‹œì‘ ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”.';
            }
        }

        async function sendAudioToServer(audioBlob) {
            const formData = new FormData();
            formData.append('file', audioBlob, 'recording.webm');
            formData.append('language', 'ko');

            // ì²˜ë¦¬ ì¤‘ í‘œì‹œ
            document.getElementById('status').className = 'status processing';
            document.getElementById('status').textContent = 'âš¡ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘...';

            try {
                // 1ë‹¨ê³„: íŒŒì¼ ì—…ë¡œë“œ ë° ì‘ì—… ì‹œì‘
                const uploadResponse = await fetch('/upload', {
                    method: 'POST',
                    body: formData
                });

                const uploadResult = await uploadResponse.json();
                
                if (uploadResult.job_id) {
                    // 2ë‹¨ê³„: ì£¼ê¸°ì ìœ¼ë¡œ ìƒíƒœ í™•ì¸
                    checkJobStatus(uploadResult.job_id);
                } else {
                    throw new Error(uploadResult.error || 'ì—…ë¡œë“œ ì‹¤íŒ¨');
                }

            } catch (error) {
                console.error('ì„œë²„ ì „ì†¡ ì˜¤ë¥˜:', error);
                document.getElementById('status').className = 'status';
                document.getElementById('status').textContent = `âŒ ì˜¤ë¥˜: ${error.message}`;
            }
        }

        async function checkJobStatus(jobId) {
            try {
                const response = await fetch(`/status/${jobId}`);
                const result = await response.json();

                if (result.status === 'completed') {
                    // ì „ì‚¬ ì™„ë£Œ!
                    const transcript = result.result.full_text;
                    const timestamp = new Date().toLocaleTimeString();
                    
                    const transcriptDiv = document.getElementById('transcript');
                    transcriptDiv.innerHTML += `
                        <div style="margin: 10px 0; padding: 10px; background: #e3f2fd; border-left: 4px solid #2196f3; border-radius: 5px;">
                            <strong>[${timestamp}]</strong> ${transcript}
                        </div>
                    `;
                    
                    // ìŠ¤í¬ë¡¤ì„ ìµœì‹  ë‚´ìš©ìœ¼ë¡œ
                    transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
                    
                    if (isRecording) {
                        document.getElementById('status').className = 'status recording';
                        document.getElementById('status').textContent = 'ğŸ”´ ë…¹ìŒ ì¤‘... ìì—°ìŠ¤ëŸ½ê²Œ ë§ì”€í•˜ì„¸ìš”!';
                    }
                    
                } else if (result.status === 'error') {
                    console.error('ì²˜ë¦¬ ì˜¤ë¥˜:', result.error);
                    document.getElementById('status').textContent = `âŒ ì²˜ë¦¬ ì˜¤ë¥˜: ${result.error}`;
                    
                } else if (result.status === 'processing') {
                    // ì•„ì§ ì²˜ë¦¬ ì¤‘ì´ë©´ 1ì´ˆ í›„ ë‹¤ì‹œ í™•ì¸
                    setTimeout(() => checkJobStatus(jobId), 1000);
                }

            } catch (error) {
                console.error('ìƒíƒœ í™•ì¸ ì˜¤ë¥˜:', error);
            }
        }

        // í˜ì´ì§€ ë¡œë“œ ì‹œ ë¸Œë¼ìš°ì € ì§€ì› í™•ì¸
        window.onload = function() {
            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                alert('ì´ ë¸Œë¼ìš°ì €ëŠ” ìŒì„± ë…¹ìŒì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìµœì‹  ë¸Œë¼ìš°ì €ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”!');
                document.getElementById('startBtn').disabled = true;
            }
        };
    </script>
</body>
</html>
```
</details>


### ğŸ‘ğŸ‘ ì¥ì ê³¼ ë‹¨ì , ì†”ì§í•˜ê²Œ ì•Œì•„ë³´ê¸°

#### ì¢‹ì€ ì  (ì´ê±´ ì§„ì§œ ìµœê³ ! âœ¨)

<div class="mermaid">
graph LR
    subgraph Advantages["Whisper ì¥ì "]
        A[ë†’ì€ ì •í™•ë„] --> E[ë›°ì–´ë‚œ ì„±ëŠ¥]
        B[99ê°œ ì–¸ì–´ ì§€ì›] --> E
        C[ë©€í‹°íƒœìŠ¤í‚¹ ê¸°ëŠ¥] --> E
        D[ì˜¤í”ˆì†ŒìŠ¤] --> F[ë†’ì€ ì ‘ê·¼ì„±]
        G[Zero-shot í•™ìŠµ] --> F
        H[ë…¸ì´ì¦ˆ ê°•ì¸ì„±] --> F
        
        E --> I[í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ]
        F --> I
    end
    
    style E fill:#c8e6c9
    style F fill:#b2dfdb
    style I fill:#a5d6a7
</div>

#### ì•„ì‰¬ìš´ ì  (ì•ìœ¼ë¡œ ë” ì¢‹ì•„ì§€ê² ì£ ? ğŸ¤”)

<div class="mermaid">
graph TD
    subgraph Limitations["ì£¼ìš” ì œí•œì‚¬í•­"]
        A[í™˜ê° í˜„ìƒ<br/>Hallucination]
        B[ë°˜ë³µì  í…ìŠ¤íŠ¸ ìƒì„±]
        C[ì–¸ì–´ë³„ ì„±ëŠ¥ í¸ì°¨]
        D[ë„ì–´ì“°ê¸° ë¬¸ì¥ë¶€í˜¸ ì˜¤ë¥˜]
    end
    
    subgraph Solutions["í•´ê²° ë°©ì•ˆ"]
        E[í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§]
        F[í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸]
        G[ì–¸ì–´ë³„ íŒŒì¸íŠœë‹]
        H[ê·œì¹™ ê¸°ë°˜ êµì •]
    end
    
    A --> E
    B --> F
    C --> G
    D --> H
    
    style A fill:#ffcdd2
    style B fill:#ffcdd2
    style C fill:#ffcdd2
    style D fill:#ffcdd2
    style E fill:#c8e6c9
    style F fill:#c8e6c9
    style G fill:#c8e6c9
    style H fill:#c8e6c9
</div>

**í™˜ê°(Hallucination) í˜„ìƒ**ì´ ë­ëƒê³ ìš”? ì¡°ìš©í•œ ë¶€ë¶„ì—ì„œ Whisperê°€ ë„ˆë¬´ ì—´ì‹¬íˆ ì¼í•œ ë‚˜ë¨¸ì§€, ìˆì§€ë„ ì•Šì€ ì†Œë¦¬ë¥¼ ìƒìƒí•´ì„œ ì§€ì–´ë‚´ëŠ” í˜„ìƒì´ì—ìš”. "ìŒ... ì—¬ê¸´ ë¬´ìŠ¨ ë§ì´ì§€? ì•„! ëŒ€ì¶© ì´ëŸ° ë‚´ìš©ì¼ ê±°ì•¼!" í•˜ê³  ì¶”ì¸¡í•˜ëŠ” ê±°ì£ . ê·€ì—½ì§€ë§Œ ê°€ë”ì€ ê³¤ë€í•˜ê² ì£ ?

### ğŸš€ ê³„ì† ì§„í™”í•˜ëŠ” Whisper, ì–´ë””ê¹Œì§€ ê°ˆê¹Œ?

WhisperëŠ” ì—¬ê¸°ì„œ ë©ˆì¶”ì§€ ì•Šì•„ìš”! ìµœê·¼ì— ë‚˜ì˜¨ **v3** ë²„ì „ì€ ì²˜ë¦¬ ì†ë„ê°€ **8ë°°**ë‚˜ ë¹¨ë¼ì§€ê³ , ë‹¤êµ­ì–´ ì„±ëŠ¥ê³¼ íƒ€ì„ìŠ¤íƒ¬í”„ ì •í™•ë„ë„ í›¨ì”¬ ì¢‹ì•„ì¡Œì–´ìš”. ê²Œë‹¤ê°€ **Turbo** ëª¨ë¸ê¹Œì§€ ë‚˜ì™€ì„œ, ì´ì œ ì‹¤ì‹œê°„ ì²˜ë¦¬ë„ ê°€ëŠ¥í•˜ë‹µë‹ˆë‹¤! ğŸš„

### ğŸš€ í”„ë¡œë•ì…˜ ë°°í¬ ê¿€íŒ! (ì‹¤ë¬´ì—ì„œ ë°”ë¡œ ì“°ëŠ” ìµœì í™”)

"ì´ì œ ì‹¤ì œ ì„œë¹„ìŠ¤ì— ì ìš©í•´ë³´ê³  ì‹¶ì–´!" í•˜ëŠ” ë¶„ë“¤ì„ ìœ„í•œ **ì‹¤ì „ ë°°í¬ ë…¸í•˜ìš°**ì˜ˆìš”!

#### âš¡ GPU ë©”ëª¨ë¦¬ ìµœì í™” (ëˆ ì•„ë¼ëŠ” ë¹„ë²•!)

<details markdown="1">
<summary>"GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•´ì„œ í° ëª¨ë¸ì„ ëª» ì¨ ğŸ˜­" í•˜ëŠ” ë¶„ë“¤ì„ ìœ„í•œ ë§ˆë²•ì˜ ì½”ë“œì˜ˆìš”!</summary>

```python
import torch
from faster_whisper import WhisperModel
import gc

class MemoryOptimizedWhisper:
    def __init__(self, model_size="large-v3"):
        self.model_size = model_size
        self.model = None
        
    def load_model_on_demand(self):
        """í•„ìš”í•  ë•Œë§Œ ëª¨ë¸ ë¡œë“œ"""
        if self.model is None:
            print("ğŸ¤– ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.model = WhisperModel(
                self.model_size, 
                device="cuda", 
                compute_type="int8",
                download_root="./models",  # ë¡œì»¬ì— ëª¨ë¸ ìºì‹œ
                local_files_only=False
            )
            print("âœ… ë¡œë“œ ì™„ë£Œ!")
    
    def unload_model(self):
        """ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ"""
        if self.model is not None:
            del self.model
            self.model = None
            gc.collect()
            torch.cuda.empty_cache()
            print("ğŸ—‘ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ!")
    
    def transcribe_batch(self, audio_files, max_concurrent=2):
        """ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ"""
        results = []
        
        try:
            self.load_model_on_demand()
            
            for i in range(0, len(audio_files), max_concurrent):
                batch = audio_files[i:i + max_concurrent]
                
                for audio_file in batch:
                    print(f"ğŸµ ì²˜ë¦¬ ì¤‘: {audio_file}")
                    segments, info = self.model.transcribe(
                        audio_file,
                        language="ko",
                        vad_filter=True,  # ë¬´ìŒ êµ¬ê°„ ì œê±°ë¡œ ì†ë„ í–¥ìƒ
                        condition_on_previous_text=False  # ë©”ëª¨ë¦¬ ì ˆì•½
                    )
                    
                    text = " ".join([segment.text for segment in segments])
                    results.append({
                        "file": audio_file,
                        "text": text,
                        "language": info.language,
                        "duration": info.duration
                    })
                
                # ë°°ì¹˜ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì •ë¦¬
                if i + max_concurrent < len(audio_files):
                    torch.cuda.empty_cache()
                    print(f"ğŸ“Š ì§„í–‰ë¥ : {min(i + max_concurrent, len(audio_files))}/{len(audio_files)}")
        
        finally:
            # ì‘ì—… ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ í•´ì œ
            self.unload_model()
        
        return results

# ì‚¬ìš©ë²•
optimizer = MemoryOptimizedWhisper("large-v3")
audio_files = ["file1.mp3", "file2.mp3", "file3.mp3"]
results = optimizer.transcribe_batch(audio_files, max_concurrent=2)
```
</details>

#### ğŸ³ Dockerë¡œ ê°„í¸ ë°°í¬! (í´ë¦­ í•œ ë²ˆìœ¼ë¡œ ë°°í¬)

<details markdown="1">
<summary>"ë³µì¡í•œ ì„¤ì • ì—†ì´ ì–´ë””ì„œë“  ë°”ë¡œ ì‹¤í–‰í•˜ê³  ì‹¶ì–´!" í•˜ëŠ” ë¶„ë“¤ì„ ìœ„í•œ Docker ì„¤ì •ì´ì—ìš”!</summary>

```dockerfile
# Dockerfile
FROM nvidia/cuda:12.1-runtime-ubuntu22.04

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
WORKDIR /app

# ìš”êµ¬ì‚¬í•­ íŒŒì¼ ë³µì‚¬ ë° ì„¤ì¹˜
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ íŒŒì¼ ë³µì‚¬
COPY . .

# ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
RUN mkdir -p /app/models

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 5000

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:5000/health || exit 1

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
CMD ["python3", "app.py"]
```


```yaml
# docker-compose.yml
version: '3.8'

services:
  whisper-api:
    build: .
    ports:
      - "5000:5000"
    volumes:
      - ./models:/app/models  # ëª¨ë¸ ìºì‹œ ì˜êµ¬ ì €ì¥
      - ./uploads:/app/uploads
      - ./outputs:/app/outputs
    environment:
      - CUDA_VISIBLE_DEVICES=0  # GPU ì„ íƒ
      - WHISPER_MODEL_SIZE=large-v3
      - MAX_WORKERS=2
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - whisper-api
    restart: unless-stopped
```



```bash
# í•œ ë²ˆì— ì‹¤í–‰!
echo "ğŸ³ Dockerë¡œ Whisper ì„œë¹„ìŠ¤ ì‹œì‘!"
docker-compose up -d

echo "ğŸŒ http://localhost ì—ì„œ í™•ì¸í•˜ì„¸ìš”!"
echo "ğŸ“Š ìƒíƒœ í™•ì¸: docker-compose logs -f"
```
</details>

#### âš–ï¸ ë¡œë“œ ë°¸ëŸ°ì‹± & ìŠ¤ì¼€ì¼ë§ (íŠ¸ë˜í”½ í­ì¦ì—ë„ ë„ë–¡ì—†ì–´!)

<details markdown="1">
<summary>"ì‚¬ìš©ìê°€ ë§ì•„ì ¸ì„œ ì„œë²„ê°€ í„°ì ¸ìš”!" í•˜ëŠ” í–‰ë³µí•œ ê³ ë¯¼ì´ ìˆëŠ” ë¶„ë“¤ì„ ìœ„í•œ ì½”ë“œì˜ˆìš”!</summary>

```python
# load_balancer.py
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor
import random
import time

class WhisperLoadBalancer:
    def __init__(self):
        # ì—¬ëŸ¬ Whisper ì„œë²„ ì¸ìŠ¤í„´ìŠ¤
        self.servers = [
            "http://whisper-server-1:5000",
            "http://whisper-server-2:5000", 
            "http://whisper-server-3:5000",
        ]
        self.server_stats = {server: {"load": 0, "last_check": 0} for server in self.servers}
    
    async def get_best_server(self):
        """ê°€ì¥ ë¶€í•˜ê°€ ì ì€ ì„œë²„ ì„ íƒ"""
        current_time = time.time()
        
        # 5ì´ˆë§ˆë‹¤ ì„œë²„ ìƒíƒœ í™•ì¸
        for server in self.servers:
            if current_time - self.server_stats[server]["last_check"] > 5:
                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.get(f"{server}/health") as response:
                            if response.status == 200:
                                data = await response.json()
                                self.server_stats[server]["load"] = data.get("current_jobs", 0)
                                self.server_stats[server]["last_check"] = current_time
                except:
                    # ì„œë²„ê°€ ì‘ë‹µí•˜ì§€ ì•Šìœ¼ë©´ ë†’ì€ ë¶€í•˜ë¡œ ì²˜ë¦¬
                    self.server_stats[server]["load"] = 999
        
        # ê°€ì¥ ë¶€í•˜ê°€ ì ì€ ì„œë²„ ë°˜í™˜
        best_server = min(self.server_stats.items(), key=lambda x: x[1]["load"])
        return best_server[0]
    
    async def transcribe_with_failover(self, audio_file, language="ko", max_retries=3):
        """ì¥ì•  ë°œìƒì‹œ ìë™ìœ¼ë¡œ ë‹¤ë¥¸ ì„œë²„ë¡œ ì¬ì‹œë„"""
        
        for attempt in range(max_retries):
            try:
                server = await self.get_best_server()
                print(f"ğŸ¯ ì‹œë„ #{attempt + 1}: {server}")
                
                data = aiohttp.FormData()
                data.add_field('file', audio_file, filename='audio.mp3')
                data.add_field('language', language)
                
                async with aiohttp.ClientSession() as session:
                    # 1ë‹¨ê³„: ì‘ì—… ì œì¶œ
                    async with session.post(f"{server}/upload", data=data) as response:
                        result = await response.json()
                        
                        if response.status == 200:
                            job_id = result["job_id"]
                            
                            # 2ë‹¨ê³„: ê²°ê³¼ ëŒ€ê¸°
                            while True:
                                await asyncio.sleep(1)  # 1ì´ˆ ëŒ€ê¸°
                                
                                async with session.get(f"{server}/status/{job_id}") as status_response:
                                    status_data = await status_response.json()
                                    
                                    if status_data["status"] == "completed":
                                        print(f"âœ… ì„±ê³µ! ({server})")
                                        return status_data["result"]
                                    elif status_data["status"] == "error":
                                        raise Exception(f"ì„œë²„ ì²˜ë¦¬ ì˜¤ë¥˜: {status_data.get('error')}")
                        else:
                            raise Exception(f"ì„œë²„ ì˜¤ë¥˜: {result.get('error')}")
            
            except Exception as e:
                print(f"âŒ ì‹œë„ #{attempt + 1} ì‹¤íŒ¨: {e}")
                if attempt == max_retries - 1:
                    raise Exception(f"ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨: {e}")
                await asyncio.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„

# ì‚¬ìš©ë²•
async def main():
    balancer = WhisperLoadBalancer()
    
    # ë™ì‹œì— ì—¬ëŸ¬ íŒŒì¼ ì²˜ë¦¬
    audio_files = ["file1.mp3", "file2.mp3", "file3.mp3"]
    
    tasks = []
    for audio_file in audio_files:
        with open(audio_file, 'rb') as f:
            task = balancer.transcribe_with_failover(f)
            tasks.append(task)
    
    # ë³‘ë ¬ ì²˜ë¦¬
    results = await asyncio.gather(*tasks)
    
    for i, result in enumerate(results):
        print(f"íŒŒì¼ {audio_files[i]}: {result['full_text'][:50]}...")

# ì‹¤í–‰
# asyncio.run(main())
```
</details>

#### ğŸ“Š ëª¨ë‹ˆí„°ë§ & ë¡œê¹… (ë¬¸ì œ ë°œìƒ ì „ì— ë¯¸ë¦¬ ì•Œì!)

<details markdown="1">
<summary>"ì„œë¹„ìŠ¤ê°€ ì˜ ëŒê³  ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì‹¶ì–´!" í•˜ëŠ” ì±…ì„ê° ìˆëŠ” ë¶„ë“¤ì„ ìœ„í•œ ì½”ë“œì˜ˆìš”!</summary>

```python
# monitoring.py
import logging
import time
import psutil
import GPUtil
from prometheus_client import Counter, Histogram, Gauge, start_http_server
from functools import wraps

# Prometheus ë©”íŠ¸ë¦­ ì •ì˜
REQUEST_COUNT = Counter('whisper_requests_total', 'Total number of requests', ['endpoint', 'status'])
REQUEST_DURATION = Histogram('whisper_request_duration_seconds', 'Request duration')
GPU_MEMORY_USAGE = Gauge('whisper_gpu_memory_percent', 'GPU memory usage percentage')
CPU_USAGE = Gauge('whisper_cpu_usage_percent', 'CPU usage percentage')
ACTIVE_JOBS = Gauge('whisper_active_jobs', 'Number of active transcription jobs')

class WhisperMonitor:
    def __init__(self, log_level=logging.INFO):
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('whisper_service.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('WhisperService')
        
        # ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘
        start_http_server(8000)
        self.logger.info("ğŸ“Š ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘: http://localhost:8000")
    
    def monitor_endpoint(self, endpoint_name):
        """ì—”ë“œí¬ì¸íŠ¸ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                start_time = time.time()
                status = "success"
                
                try:
                    result = func(*args, **kwargs)
                    self.logger.info(f"âœ… {endpoint_name} ì„±ê³µ")
                    return result
                except Exception as e:
                    status = "error"
                    self.logger.error(f"âŒ {endpoint_name} ì‹¤íŒ¨: {str(e)}")
                    raise
                finally:
                    duration = time.time() - start_time
                    REQUEST_COUNT.labels(endpoint=endpoint_name, status=status).inc()
                    REQUEST_DURATION.observe(duration)
                    
                    self.logger.info(f"â±ï¸ {endpoint_name} ì²˜ë¦¬ ì‹œê°„: {duration:.2f}ì´ˆ")
            
            return wrapper
        return decorator
    
    def update_system_metrics(self):
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        try:
            # CPU ì‚¬ìš©ë¥ 
            cpu_percent = psutil.cpu_percent(interval=1)
            CPU_USAGE.set(cpu_percent)
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
            try:
                gpus = GPUtil.getGPUs()
                if gpus:
                    gpu_memory = gpus[0].memoryUtil * 100
                    GPU_MEMORY_USAGE.set(gpu_memory)
                    
                    if gpu_memory > 90:
                        self.logger.warning(f"ğŸš¨ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±: {gpu_memory:.1f}%")
            except:
                self.logger.warning("GPU ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ë¡œê·¸ ì¶œë ¥
            self.logger.debug(f"ğŸ’» CPU: {cpu_percent:.1f}%")
            
        except Exception as e:
            self.logger.error(f"ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {str(e)}")
    
    def health_check(self):
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        status = {
            "status": "healthy",
            "timestamp": time.time(),
            "system": {}
        }
        
        try:
            # ë©”ëª¨ë¦¬ í™•ì¸
            memory = psutil.virtual_memory()
            status["system"]["memory_percent"] = memory.percent
            
            # ë””ìŠ¤í¬ í™•ì¸
            disk = psutil.disk_usage('/')
            status["system"]["disk_percent"] = (disk.used / disk.total) * 100
            
            # ì„ê³„ì¹˜ í™•ì¸
            if memory.percent > 85:
                status["status"] = "warning"
                status["warnings"] = status.get("warnings", [])
                status["warnings"].append(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory.percent:.1f}%")
            
            if status["system"]["disk_percent"] > 85:
                status["status"] = "warning"
                status["warnings"] = status.get("warnings", [])
                status["warnings"].append(f"ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {status['system']['disk_percent']:.1f}%")
            
        except Exception as e:
            status["status"] = "error"
            status["error"] = str(e)
            self.logger.error(f"í—¬ìŠ¤ì²´í¬ ì˜¤ë¥˜: {str(e)}")
        
        return status

# ì‚¬ìš©ë²• ì˜ˆì œ
monitor = WhisperMonitor()

@monitor.monitor_endpoint("transcribe")
def transcribe_audio(audio_file):
    """ëª¨ë‹ˆí„°ë§ì´ ì ìš©ëœ ì „ì‚¬ í•¨ìˆ˜"""
    ACTIVE_JOBS.inc()  # í™œì„± ì‘ì—… ìˆ˜ ì¦ê°€
    
    try:
        # ì‹¤ì œ ì „ì‚¬ ì‘ì—…
        model = WhisperModel("base")
        segments, info = model.transcribe(audio_file)
        result = " ".join([segment.text for segment in segments])
        
        monitor.logger.info(f"ğŸ“ ì „ì‚¬ ì™„ë£Œ: {len(result)}ì")
        return result
        
    finally:
        ACTIVE_JOBS.dec()  # í™œì„± ì‘ì—… ìˆ˜ ê°ì†Œ

# Flask ì•±ì— ì ìš©
from flask import Flask, jsonify

app = Flask(__name__)

@app.route('/health')
def health():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return jsonify(monitor.health_check())

@app.route('/metrics')
def metrics():
    """ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ (Prometheus í˜•ì‹)"""
    monitor.update_system_metrics()
    return "ë©”íŠ¸ë¦­ì´ http://localhost:8000 ì—ì„œ ì œê³µë©ë‹ˆë‹¤."

if __name__ == '__main__':
    monitor.logger.info("ğŸš€ Whisper ì„œë¹„ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘!")
    app.run(host='0.0.0.0', port=5000)
```
</details>

<details markdown="1">
<summary> #### ğŸ”§ ì„±ëŠ¥ íŠœë‹ ì²´í¬ë¦¬ìŠ¤íŠ¸ (ë§ˆì§€ë§‰ ìµœì í™”!)</summary>

```python
# performance_tips.py

class WhisperPerformanceTips:
    """
    ğŸš€ Whisper ì„±ëŠ¥ ìµœì í™” ì™„ë²½ ê°€ì´ë“œ
    """
    
    @staticmethod
    def optimize_model_loading():
        """ëª¨ë¸ ë¡œë”© ìµœì í™”"""
        print("ğŸ“š ëª¨ë¸ ë¡œë”© ìµœì í™” íŒ:")
        print("âœ… 1. ë¡œì»¬ì— ëª¨ë¸ ìºì‹œí•˜ê¸°")
        print("âœ… 2. í•„ìš”í•  ë•Œë§Œ ëª¨ë¸ ë¡œë“œ")
        print("âœ… 3. ì‘ì€ ëª¨ë¸ë¶€í„° ì‹œì‘í•´ì„œ ë‹¨ê³„ì  ì—…ê·¸ë ˆì´ë“œ")
        print("âœ… 4. INT8 ì–‘ìí™” ì ê·¹ í™œìš©")
        
        return {
            "cache_dir": "./models",
            "compute_type": "int8", 
            "local_files_only": True
        }
    
    @staticmethod
    def optimize_inference():
        """ì¶”ë¡  ìµœì í™”"""
        print("\nâš¡ ì¶”ë¡  ì†ë„ ìµœì í™”:")
        print("âœ… 1. VAD í•„í„°ë¡œ ë¬´ìŒ êµ¬ê°„ ì œê±°")
        print("âœ… 2. condition_on_previous_text=Falseë¡œ ë©”ëª¨ë¦¬ ì ˆì•½")
        print("âœ… 3. ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ë¡œ í’ˆì§ˆ í–¥ìƒ")
        print("âœ… 4. ë°°ì¹˜ ì²˜ë¦¬ë¡œ íš¨ìœ¨ì„± ì¦ëŒ€")
        
        return {
            "vad_filter": True,
            "condition_on_previous_text": False,
            "beam_size": 1,  # ë¹ ë¥¸ ì²˜ë¦¬
            "temperature": 0.0  # ì¼ê´€ì„± í–¥ìƒ
        }
    
    @staticmethod
    def memory_management():
        """ë©”ëª¨ë¦¬ ê´€ë¦¬"""
        print("\nğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™”:")
        print("âœ… 1. ì‚¬ìš© í›„ torch.cuda.empty_cache()")
        print("âœ… 2. gc.collect()ë¡œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜")
        print("âœ… 3. í° íŒŒì¼ì€ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬")
        print("âœ… 4. ëª¨ë¸ ì–¸ë¡œë“œ í›„ ì¬ë¡œë“œ")
        
        import torch
        import gc
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜
        def cleanup():
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        return cleanup
    
    @staticmethod
    def production_checklist():
        """í”„ë¡œë•ì…˜ ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸"""
        checklist = [
            "ğŸ” ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§ ì„¤ì •",
            "ğŸš¨ ì—ëŸ¬ í•¸ë“¤ë§ ë° ì¬ì‹œë„ ë¡œì§",
            "âš–ï¸ ë¡œë“œ ë°¸ëŸ°ì‹± êµ¬ì„±",
            "ğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘",
            "ğŸ”’ ë³´ì•ˆ ì„¤ì • (íŒŒì¼ ì—…ë¡œë“œ ì œí•œ ë“±)",
            "ğŸ’¾ ìºì‹œ ì „ëµ ìˆ˜ë¦½",
            "ğŸ³ ì»¨í…Œì´ë„ˆí™” ë° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜",
            "ğŸ“ˆ ìŠ¤ì¼€ì¼ë§ ì „ëµ",
            "ğŸ”„ CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì¶•",
            "ğŸ“‹ ë¬¸ì„œí™” ë° API ìŠ¤í™"
        ]
        
        print("\nğŸ“‹ í”„ë¡œë•ì…˜ ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸:")
        for item in checklist:
            print(f"  {item}")
        
        return checklist

# ì„±ëŠ¥ ìµœì í™” ì ìš©
if __name__ == "__main__":
    tips = WhisperPerformanceTips()
    
    model_config = tips.optimize_model_loading()
    inference_config = tips.optimize_inference()
    cleanup_func = tips.memory_management()
    checklist = tips.production_checklist()
    
    print(f"\nğŸ‰ ìµœì í™” ì„¤ì • ì™„ë£Œ! ì´ì œ ì—¬ëŸ¬ë¶„ì˜ Whisperê°€ ë²ˆê°œì²˜ëŸ¼ ë¹¨ë¼ì§ˆ ê±°ì˜ˆìš”! âš¡")
```
</details>

#### Whisperì˜ ë¯¸ë˜ëŠ”?

<div class="mermaid">
timeline
    title Whisper ë°œì „ ë¡œë“œë§µ
    
    2022 : Whisper v1 ì¶œì‹œ
         : 68ë§Œ ì‹œê°„ í•™ìŠµ
         : 99ê°œ ì–¸ì–´ ì§€ì›
    
    2023 : Whisper v2 ê°œì„ 
         : ì„±ëŠ¥ ìµœì í™”
         : ì•ˆì •ì„± í–¥ìƒ
    
    2024 : Whisper v3 & Turbo
         : 8ë°° ì†ë„ ê°œì„ 
         : ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥
    
    2025 : ì˜ˆìƒ ë°œì „ ë°©í–¥
         : ë” ë§ì€ ì–¸ì–´ ì§€ì›
         : ì‹¤ì‹œê°„ ë™ì‹œí†µì—­ ê°•í™”
         : ì—£ì§€ ë””ë°”ì´ìŠ¤ ìµœì í™”
         : ë”ìš± ì •í™•í•œ ê°ì • ì¸ì‹
</div>

## ğŸ‰ ê²°ë¡ : Whisper, ì´ì œ ì„ íƒì´ ì•„ë‹Œ í•„ìˆ˜!

OpenAI WhisperëŠ” ìŒì„± ì¸ì‹ ë¶„ì•¼ì˜ 'ê²Œì„ ì²´ì¸ì €' ê·¸ ìì²´ì…ë‹ˆë‹¤.

1.  **ë²”ìš©ì„±**: 99ê°œ ì–¸ì–´ë¡œ ì „ ì„¸ê³„ë¥¼ ë¬´ëŒ€ë¡œ!
2.  **í¸ì˜ì„±**: ì˜¤í”ˆì†ŒìŠ¤ë¡œ ëˆ„êµ¬ë‚˜ ì‰½ê²Œ!
3.  **í™•ì¥ì„±**: ë‚´ ì…ë§›ëŒ€ë¡œ íŠœë‹í•´ì„œ ìµœì í™”!
4.  **ì‹¤ìš©ì„±**: ì´ë¯¸ ìˆ˜ë§ì€ ì„œë¹„ìŠ¤ì—ì„œ ê²€ì¦ëœ ì„±ëŠ¥!

íŠ¹íˆ í•œêµ­ì–´ì—ì„œë„ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ë§Œí¼, ì—¬ëŸ¬ë¶„ì˜ í”„ë¡œì íŠ¸ì— ë‚ ê°œë¥¼ ë‹¬ì•„ì¤„ ê°•ë ¥í•œ ë¬´ê¸°ê°€ ë  ê±°ì˜ˆìš”. íŒŒì¸íŠœë‹ê³¼ ì–‘ìí™”ê¹Œì§€ ê³ë“¤ì¸ë‹¤ë©´, ìƒìƒë§Œ í•˜ë˜ ìŒì„± ì¸ì‹ ì†”ë£¨ì…˜ì„ ì§ì ‘ ë§Œë“¤ ìˆ˜ ìˆë‹µë‹ˆë‹¤.

ì•ìœ¼ë¡œ ë˜ ì–´ë–»ê²Œ ìš°ë¦¬ë¥¼ ë†€ë¼ê²Œ í• ì§€, ê³„ì†í•´ì„œ ì§€ì¼œë³´ìê³ ìš”!

---

## ì°¸ê³  ìë£Œ

- [Whisper ëª¨ë¸ ë¶„ì„ - TuringPost](https://turingpost.co.kr/p/topic-15-openai-whisper)
- [Whisper API ì‚¬ìš©ë²• - Magic AI Prompts](https://www.magicaiprompts.com/docs/gpt-chatbot/whisper-speech-recognition-api/)
- [Whisper íŒŒì¸íŠœë‹ ê°€ì´ë“œ](https://ysg2997.tistory.com/53)
- [Whisper ì–‘ìí™” ìµœì í™”](https://medium.com/@daniel-klitzke/quantizing-openais-whisper-with-the-huggingface-optimum-library-30-faster-inference-64-36d9815190e0)
- [ì–‘ìí™” ì„±ëŠ¥ ë¹„êµ ì—°êµ¬](https://www.chatpaper.ai/ko/dashboard/paper/3074c9d4-da0c-4444-85b2-23949f26829c)
